{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b21fe2-0038-4875-9aa5-7480519c7a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] Le processus ne peut pas accéder au fichier car ce fichier est utilisé par un autre processus: 'c:\\\\users\\\\talelbm\\\\appdata\\\\local\\\\anaconda3\\\\lib\\\\site-packages\\\\saspy\\\\java\\\\iomclient\\\\log4j-1.2-api-2.12.4.jar'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U --quiet langchain langgraph langchain_google_genai\n",
    "%pip install -U --quiet saspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb9a227-e221-4ba6-9ba2-eb12593744da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import os\n",
    "import getpass\n",
    "import re\n",
    "from collections import deque\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "import pandas as pd\n",
    "from gradio_client import Client\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import google.generativeai as genai\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "import saspy\n",
    "import json\n",
    "# Constants\n",
    "END = \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28030c1d-e4fc-4419-b9cf-3e373aa794b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436c7dfa-970a-4ee1-b099-71f97dc6b523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY ········\n"
     ]
    }
   ],
   "source": [
    "def _set_if_undefined(var: str) -> None:\n",
    "    \"\"\"Set environment variable if not already defined.\"\"\"\n",
    "    if os.environ.get(var):\n",
    "        return\n",
    "    os.environ[var] = getpass.getpass(var)\n",
    "\n",
    "_set_if_undefined(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "846d0b34-1a3a-42c1-a717-4ee5e174289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"\"\"%(asctime)s - %(levelname)s - %(message)s\n",
    "-------------------------------------------\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0118aa83-4531-4b17-b037-c0b1bfe05c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    metadata_file_path = 'metadata_dict.json'  # Assuming the file is in the same directory\n",
    "    with open(metadata_file_path, 'r', encoding='utf-8') as f:\n",
    "        metadata_dict = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Error: Metadata file not found at {metadata_file_path}\")\n",
    "    raise\n",
    "except json.JSONDecodeError:\n",
    "    logging.error(f\"Error: Invalid JSON format in {metadata_file_path}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a857508-0c9e-4a41-8f11-99a80e9e602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sas = saspy.SASsession(cfgname='iomwin', cfgfile='sascfg.py')\n",
    "    logging.info(\"SAS session initialized successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing SAS session: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5643d8b6-4ca6-41d2-90e5-caa75fcf868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataVerifier:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def verify_metadata(self, metadata: str, question: str, original_metadata: dict) -> tuple[bool, str]:\n",
    "        logging.info(\"Verifying metadata...\")\n",
    "        verification_prompt = f\"\"\"Act as a critical data analyst reviewing metadata selection. \n",
    "Verify if the selected metadata is contains all the sufficient and necessary databases and columns for answering the query. do not go further than that.\n",
    "if the database is sufficient to answer the query, approve it.\n",
    "\n",
    "Question: {question}\n",
    "Selected Metadata (to be analyzed carefully):\n",
    "{metadata}\n",
    "\n",
    "Complete Available Metadata (to be analyzed carefully):\n",
    "{original_metadata}\n",
    "\n",
    "Check for these specific issues:\n",
    "1. Missing Essential Columns:\n",
    "   <redacted for privacy reasons>\n",
    "\n",
    "2. Completeness Check:\n",
    "   - All columns necessary for the query's calculations\n",
    "   - All columns needed for filtering conditions\n",
    "   - All columns needed for grouping or aggregation\n",
    "   - All columns needed for the final output\n",
    "\n",
    "3. Business Rules Verification:\n",
    "   <redacted for privacy reasons>\n",
    "\n",
    "Respond with a string containing:\n",
    "    approved: boolean (\"approved: True\" or \"approved : False\"),\n",
    "    issues: [list of specific issues found, strings with \"...\" format],\n",
    "    missing columns: [list of essential missing columns, strings with \"...\" format],\n",
    "    suggestions: [specific suggestions for improvement, strings with \"...\" format],\n",
    "    criticism: \"detailed explanation of why the metadata is or isn't optimal\"\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = self.model.generate_content(verification_prompt)\n",
    "            logging.info(\"Metadata verification completed.\")\n",
    "            return \"approved: True\" in response.text, response.text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during metadata verification: {e}\")\n",
    "            return False, str(e)\n",
    "\n",
    "class CachedMetadata:\n",
    "    _instance = None\n",
    "\n",
    "    def __init__(self, question, metadata_dict):\n",
    "        if CachedMetadata._instance is not None:\n",
    "            logging.warning(\"Attempted to create a new instance of Singleton CachedMetadata.\")\n",
    "            raise Exception(\"This class is a singleton!\")\n",
    "\n",
    "        self.model_gemini = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "        self.question = question\n",
    "        self.metadata_dict = metadata_dict\n",
    "        self._metadata = None\n",
    "        self._verifier = MetadataVerifier(self.model_gemini)\n",
    "        self._generate_metadata()\n",
    "\n",
    "    @classmethod\n",
    "    def get_instance(cls, question=None, metadata_dict=None):\n",
    "        if cls._instance is None:\n",
    "            if question is None or metadata_dict is None:\n",
    "                logging.error(\"Metadata dictionary and question must be provided for the first initialization.\")\n",
    "                raise ValueError(\n",
    "                    \"Metadata dictionary and question must be provided for the first initialization.\")\n",
    "            logging.info(\"Creating new instance of CachedMetadata.\")\n",
    "            cls._instance = CachedMetadata(question, metadata_dict)\n",
    "        else:\n",
    "            logging.info(\"Returning existing instance of CachedMetadata.\")\n",
    "        return cls._instance\n",
    "\n",
    "    def _generate_metadata(self):\n",
    "        logging.info(\"Generating metadata...\")\n",
    "        prompt = f\"\"\"Act as an expert data analyst. Given a query and a database's metadata, identify the smallest set of databases \n",
    "        and columns necessary to answer the query accurately. Ensure that only the essential databases and columns are included, and \n",
    "        consider any required joins or relationships between tables. \n",
    "        Follow these specific rules:\n",
    "        - give back a string in this format : \n",
    "        \"\"\n",
    "        relevant_database1 : relevant_database1_description\n",
    "            relevant_column11 : relevant_column11_type\n",
    "                                relevant_column11_description\n",
    "                                relevant_column11_values (if they exist)\n",
    "            ...\n",
    "        \"\"\n",
    "        \n",
    "        <redacted for privacy reasons>\n",
    "        \n",
    "        Provide the selected subset of metadata as a string, including only the necessary databases and their associated columns, without any additional explanations.\"\n",
    "        Here is the question: {self.question}\n",
    "        Here is the metadata: {self.metadata_dict}\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model_gemini.generate_content(prompt)\n",
    "            self._metadata = response.text\n",
    "            logging.info(\"Metadata generated.\")\n",
    "\n",
    "            # Verify metadata immediately\n",
    "            is_approved, verification_result = self._verifier.verify_metadata(\n",
    "                self._metadata, self.question, self.metadata_dict\n",
    "            )\n",
    "            if not is_approved:\n",
    "                logging.warning(\"Initial metadata not approved. Regenerating...\")\n",
    "                self._regenerate_metadata(verification_result)\n",
    "            return self._metadata\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during metadata generation: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _regenerate_metadata(self, verification_result):\n",
    "        logging.info(\"Regenerating metadata...\")\n",
    "        regeneration_prompt = f\"\"\"Previous metadata selection was inadequate. Please regenerate the metadata selection addressing these issues:\n",
    "        task:\n",
    "        Act as an expert data analyst. Given a query and a database's metadata, identify the smallest set of databases \n",
    "        and columns necessary to answer the query accurately. Ensure that only the essential databases and columns are included, and \n",
    "        consider any required joins or relationships between tables. \n",
    "        Follow these specific rules:\n",
    "        - give back a string in this format : \n",
    "        \"\"\n",
    "        relevant_database1 : relevant_database1_description\n",
    "            relevant_column11 : relevant_column11_type\n",
    "                                relevant_column11_description\n",
    "                                relevant_column11_values (if they exist)\n",
    "            ...\n",
    "        \"\"\n",
    "        <redacted for privacy reasons>\n",
    "        \n",
    "        Provide the selected subset of metadata as a string, including only the necessary databases and their associated columns, without any additional explanations.\n",
    "        The old response:\n",
    "        {self._metadata}\n",
    "        The full critique:\n",
    "        {verification_result}\n",
    "\n",
    "        Original question: {self.question}\n",
    "        Available metadata: {self.metadata_dict}\"\"\"\n",
    "        try:\n",
    "            response = self.model_gemini.generate_content(regeneration_prompt)\n",
    "            self._metadata = response.text\n",
    "            logging.info(\"Metadata regenerated.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during metadata regeneration: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_metadata(self):\n",
    "        return self._metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2639e0ae-4368-482b-abd2-f96b31deec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "def execute_sas_code(sas_code: str) -> str:\n",
    "    \"\"\"Execute SAS code and return log.\"\"\"\n",
    "    logging.info(\"Executing SAS code...\")\n",
    "    try:\n",
    "        result = sas.submit(sas_code)\n",
    "        logging.info(\"SAS code executed. Returning log.\")\n",
    "        return result['LOG']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing SAS code: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "def extract_data(sas_code: str, client) -> None:\n",
    "    \"\"\"Execute SAS code and save results to Excel.\"\"\"\n",
    "    logging.info(\"Extracting data...\")\n",
    "    try:\n",
    "        sas.submit(sas_code)\n",
    "        # Assuming 'client' is defined globally or passed as a parameter\n",
    "        df_name_result = client.predict(\n",
    "            query=f\"Extract final dataframe name from, dont include 'WORK.', just the raw name:\\n{sas_code}\",\n",
    "            api_name=\"/generation_code\"\n",
    "        )\n",
    "\n",
    "        if df_name_result:\n",
    "            df_name = df_name_result[0]\n",
    "            df = sas.sd2df(df_name)\n",
    "            if not df.empty:\n",
    "                excel_file = f\"{df_name}.xlsx\"\n",
    "                df.to_excel(excel_file, index=False)\n",
    "                logging.info(f\"Data extracted to {excel_file}\")\n",
    "            else:\n",
    "                logging.warning(f\"Dataframe {df_name} is empty.\")\n",
    "        else:\n",
    "            logging.warning(\"Could not determine dataframe name for extraction.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e6cb8f-162c-419b-8b41-d13734601a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt Templates and System Messages ---\n",
    "def get_thinker_system_prompt(necessary_metadata):\n",
    "    return f\"\"\"\n",
    "You are StarThinker, a strategic module of the StarData AI coding agent.\n",
    "You have a basic understanding of actuary line of thinking, especially concerning automobile insurance.\n",
    "Your primary role is to analyze a given data query and devise multiple, distinct strategies for answering it using SAS code. You do not generate code, only high-level strategies.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "- Given a data query and metadata, generate distinct and exhaustive strategies outlining how to approach the problem using the available data in the SAS databases.\n",
    "- Strategies should be concise (1-2 sentences each) and explore different logical paths to the solution.\n",
    "- Consider various ways to utilize the provided metadata, which includes database names, descriptions, column names, descriptions, types, and potential values.\n",
    "- Dynamically determine the appropriate number of strategies based on the complexity of the query. Stop generating new strategies if you deem them to be sufficient and exhaustive.\n",
    "- give your response in this format :\n",
    "    'strategy 1 : <details of this strategy>\\n\n",
    "    strategy 2 : <details of this strategy>\\n\n",
    "    ...'\n",
    "**Key Considerations from StarData:**\n",
    "\n",
    "<redacted for privacy reasons>\n",
    "\n",
    "**Metadata:**\n",
    "\n",
    "{necessary_metadata}\n",
    "\"\"\"\n",
    "\n",
    "def get_solver_system_prompt(necessary_metadata):\n",
    "    return f\"\"\"\n",
    "You are StarSolver, a code generation module of the StarData AI coding agent.\n",
    "You have a basic understanding of actuary line of thinking, especially concerning automobile insurance.\n",
    "You are an expert in SAS programming. Your task is to generate SAS code that accurately answers data queries based on provided strategies and metadata.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "- Given a data query and a specific strategy, generate a complete and executable SAS code solution.\n",
    "- The code should be syntactically correct and produce the desired output based on the query and strategy.\n",
    "- Output ONLY the SAS code, without any explanations or additional text.\n",
    "- Save all new dataframes in the WORK library (temporary).\n",
    "- When doing union joins, select only relevant columns.\n",
    "- Adhere to all coding instructions and guidelines specified below.\n",
    "\n",
    "**Coding Instructions from StarData:**\n",
    "\n",
    "<redacted for privacy reasons>\n",
    "\n",
    "**Metadata:**\n",
    "\n",
    "{necessary_metadata}\n",
    "\"\"\"\n",
    "\n",
    "def get_debugger_system_prompt(necessary_metadata):\n",
    "    return f\"\"\"\n",
    "You are StarDebugger, a code refinement module of the StarData AI coding agent.\n",
    "You have a basic understanding of actuary line of thinking, especially concerning automobile insurance.\n",
    "You are an expert in SAS programming and debugging. Your task is to refine and improve SAS code solutions based on provided feedback.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "- Given a SAS code solution, and feedback (which may include error messages, requirement shortcomings, or suggestions), generate a corrected and improved version of the code.\n",
    "- The refined code should address all issues mentioned in the feedback and produce the desired output according to the original query.\n",
    "- Output ONLY the refined SAS code, without any explanations or additional text.\n",
    "\n",
    "**Coding and Debugging Instructions from StarData:**\n",
    "<redacted for privacy reasons>\n",
    "**Metadata:**\n",
    "\n",
    "{necessary_metadata}\n",
    "\"\"\"\n",
    "\n",
    "def get_critic_system_prompt(necessary_metadata):\n",
    "    return f\"\"\"\n",
    "You are StarCritic, a solution evaluator module of the StarData AI coding agent.\n",
    "You have a basic understanding of actuary line of thinking, especially concerning automobile insurance.\n",
    "You are an expert in SAS programming and analysis. Your task is to evaluate SAS code solutions generated by the Solver agent, provide feedback, and assess their suitability for refinement or acceptance.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "- Given a SAS code solution, its corresponding strategy, and the original query, evaluate the solution's correctness, adherence to the strategy, and overall quality.\n",
    "- Provide a numerical score (critic_score) that reflects the solution's quality and potential for improvement.\n",
    "- Generate specific feedback (feedback) that identifies errors, missing requirements, and areas for improvement.\n",
    "- Determine whether the solution should be refined, aborted, or accepted based on your evaluation.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "\n",
    "- **Correctness:** Does the code execute without errors? Does it produce the expected output for visible test cases?\n",
    "- **Strategy Adherence:** Does the code effectively implement the given strategy? Does it logically follow the strategy and use appropriate data structures and algorithms?\n",
    "- **Robustness:** Is the solution likely to be correct for unseen test cases? Does it handle potential edge cases and demonstrate a general understanding of the problem?\n",
    "- **Requirements Fulfillment:** Does the code meet all the requirements stated in the original query? Are there any missing functionalities or discrepancies?\n",
    "\n",
    "**Decision Logic:**\n",
    "\n",
    "- **Refine:** If the solution has errors, fails to meet requirements, or does not fully adhere to the strategy, it should be refined.\n",
    "- **Abort:** If the solution has major flaws, scores very low on the evaluation criteria, or is unlikely to be improved with further refinement, it should be aborted.\n",
    "- **Accept:** If the solution passes all visible test cases, adheres to the strategy, meets all requirements, and is deemed robust, it should be accepted as the final solution.\n",
    "\n",
    "**Additional Instructions:**\n",
    "\n",
    "<redacted for privacy reasons>\n",
    "\n",
    "**Metadata:**\n",
    "\n",
    "{necessary_metadata}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce92129-62ab-45d1-b9c9-d036775ff50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agent Classes ---\n",
    "class Thinker:\n",
    "    def __init__(self, llm, necessary_metadata):\n",
    "        self.llm = llm\n",
    "        self.necessary_metadata = necessary_metadata\n",
    "        self.strategies_cache: Dict[str, List[str]] = {}\n",
    "        self.reflections_cache: Dict[Tuple[str, str, str, str], List[str]] = {}\n",
    "\n",
    "    def generate_strategies(self, question: str, max_strategies: int = 5, previous_strategies: List[str] = None) -> List[str]:\n",
    "        \"\"\"Generates multiple strategies for solving the coding problem autoregressively.\"\"\"\n",
    "        logging.info(f\"Generating strategies for question: {question}\")\n",
    "        if question in self.strategies_cache:\n",
    "            logging.info(f\"Using cached strategies for question: {question}\")\n",
    "            return self.strategies_cache[question]\n",
    "\n",
    "        strategies = []\n",
    "        \n",
    "        if previous_strategies is not None:\n",
    "            strategies = previous_strategies\n",
    "\n",
    "        for i in range(max_strategies):\n",
    "            prompt = self._build_strategy_prompt(question, strategies)\n",
    "            try:\n",
    "                response = (prompt | self.llm | StrOutputParser()).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"necessary_metadata\": self.necessary_metadata,\n",
    "                    \"previous_strategies\": \"\\n\".join(strategies)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during strategy generation: {e}\")\n",
    "                break\n",
    "\n",
    "            new_strategy = self._extract_strategy(response)\n",
    "            if not new_strategy or new_strategy.lower() == \"no further strategies.\":\n",
    "                logging.info(\"No further strategies generated.\")\n",
    "                break\n",
    "\n",
    "            strategies.append(new_strategy)\n",
    "            logging.info(f\"Generated strategy {i+1}: {new_strategy}\")\n",
    "\n",
    "        self.strategies_cache[question] = strategies\n",
    "        return strategies\n",
    "\n",
    "    def _build_strategy_prompt(self, question: str, previous_strategies: List[str]) -> ChatPromptTemplate:\n",
    "        \"\"\"Builds the prompt for generating the next strategy.\"\"\"\n",
    "        system_prompt = get_thinker_system_prompt(self.necessary_metadata)\n",
    "\n",
    "        if previous_strategies:\n",
    "            system_prompt += \"\\n\\nPrevious Strategies:\\n\" + \"\\n\".join(\n",
    "                [f\"{i+1}. {strategy}\" for i, strategy in enumerate(previous_strategies)]\n",
    "            )\n",
    "        system_prompt += \"\\n\\nGenerate the next strategy, or write 'No further strategies.' if no further strategies can be generated.\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", f\"Question: {question}\\nStrategy:\")\n",
    "        ])\n",
    "        return prompt\n",
    "\n",
    "    def _extract_strategy(self, response: str) -> str:\n",
    "        \"\"\"Extracts a strategy from the LLM's response.\"\"\"\n",
    "        # Use regex to find the strategy after 'strategy x :'\n",
    "        match = re.search(r\"strategy \\d+ : (.*)\", response, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def generate_reflections(self, question: str, solution: str, feedback: str, log: str, strategy: str,\n",
    "                             num_reflections: int = 3) -> List[str]:\n",
    "        \"\"\"Generates reflections on the code autoregressively.\"\"\"\n",
    "        logging.info(f\"Generating reflections for question: {question}\")\n",
    "        cache_key = (question, solution, feedback, log)\n",
    "        if cache_key in self.reflections_cache:\n",
    "            logging.info(f\"Using cached reflections for question: {question}\")\n",
    "            return self.reflections_cache[cache_key]\n",
    "\n",
    "        reflections = []\n",
    "        for _ in range(num_reflections):\n",
    "            prompt = self._build_reflection_prompt(question, solution, feedback, log, strategy, reflections)\n",
    "            try:\n",
    "                response = (prompt | self.llm | StrOutputParser()).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"solution\": solution,\n",
    "                    \"feedback\": feedback,\n",
    "                    \"log\": log,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"necessary_metadata\": self.necessary_metadata,\n",
    "                    \"previous_reflections\": \"\\n\".join(reflections)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during reflection generation: {e}\")\n",
    "                break\n",
    "\n",
    "            new_reflection = response.strip()\n",
    "            if not new_reflection or new_reflection.lower() == \"no further reflections.\":\n",
    "                logging.info(\"No further reflections generated.\")\n",
    "                break\n",
    "            reflections.append(new_reflection)\n",
    "            logging.info(f\"Generated reflection: {new_reflection}\")\n",
    "\n",
    "        self.reflections_cache[cache_key] = reflections\n",
    "        return reflections\n",
    "\n",
    "    def _build_reflection_prompt(self, question: str, solution: str, feedback: str, log: str, strategy: str,\n",
    "                                 previous_reflections: List[str]) -> ChatPromptTemplate:\n",
    "        \"\"\"Builds the prompt for generating the next reflection.\"\"\"\n",
    "        system_prompt = get_thinker_system_prompt(self.necessary_metadata)\n",
    "        system_prompt += f\"\\n\\nStrategy used: {strategy}\"\n",
    "        system_prompt += \"\\n\\nYour task is to generate reflections on the following SAS code solution, considering the feedback and log provided. \"\n",
    "        system_prompt += \"Reflections should be concise and focus on identifying areas for improvement or issues in the code.\"\n",
    "\n",
    "        if previous_reflections:\n",
    "            system_prompt += \"\\n\\nPrevious Reflections:\\n\" + \"\\n\".join(\n",
    "                [f\"{i+1}. {reflection}\" for i, reflection in enumerate(previous_reflections)]\n",
    "            )\n",
    "            system_prompt += \"\\n\\nGenerate the next reflection, or write 'No further reflections.' if no further reflections are needed\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", f\"Given the question: '{question}', the strategy: '{strategy}', the following SAS code solution: \\n\"\n",
    "                     f\"```sas\\n{solution}\\n```\\n\"\n",
    "                     f\"and the feedback from the Critic: '{feedback}',\\n\"\n",
    "                     f\"and the log from the code execution : '{log}', \\n\"\n",
    "                     f\"please generate reflections to guide the code refinement process.\")\n",
    "        ])\n",
    "        return prompt\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self, llm, necessary_metadata):\n",
    "        self.llm = llm\n",
    "        self.necessary_metadata = necessary_metadata\n",
    "        self.solutions_cache: Dict[Tuple[str, str], str] = {}\n",
    "\n",
    "    def generate_solution(self, question: str, strategy: str) -> str:\n",
    "        \"\"\"Generates an initial SAS code solution based on the given strategy.\"\"\"\n",
    "        logging.info(f\"Generating solution for question: {question} with strategy: {strategy}\")\n",
    "        cache_key = (question, strategy)\n",
    "        if cache_key in self.solutions_cache:\n",
    "            logging.info(f\"Using cached solution for question: {question} and strategy: {strategy}\")\n",
    "            return self.solutions_cache[cache_key]\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", get_solver_system_prompt(self.necessary_metadata)),\n",
    "            (\"user\", f\"Given the strategy: '{strategy}', please write SAS code to answer this question: {question}\")\n",
    "        ])\n",
    "        try:\n",
    "            response = (prompt | self.llm | StrOutputParser()).invoke({\n",
    "                \"question\": question,\n",
    "                \"strategy\": strategy,\n",
    "                \"necessary_metadata\": self.necessary_metadata\n",
    "            })\n",
    "            solution = self._extract_code(response)\n",
    "            logging.info(f\"Generated solution:\\n{solution}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during solution generation: {e}\")\n",
    "            solution = \"\"\n",
    "\n",
    "        self.solutions_cache[cache_key] = solution\n",
    "        return solution\n",
    "\n",
    "    def _extract_code(self, response: str) -> str:\n",
    "        \"\"\"Extracts the SAS code from the LLM's response.\"\"\"\n",
    "        return response.strip()\n",
    "\n",
    "class Debugger:\n",
    "    def __init__(self, llm, necessary_metadata):\n",
    "        self.llm = llm\n",
    "        self.necessary_metadata = necessary_metadata\n",
    "        self.debugged_solutions_cache: Dict[Tuple[str, str, str], str] = {}\n",
    "\n",
    "    def generate_refinement(self, question: str, solution: str, reflections: str) -> str:\n",
    "        \"\"\"Refines the given SAS code solution based on reflections.\"\"\"\n",
    "        logging.info(f\"Generating refinement for question: {question}\")\n",
    "        cache_key = (question, solution, reflections)\n",
    "        if cache_key in self.debugged_solutions_cache:\n",
    "            logging.info(f\"Using cached refined solution for question: {question}\")\n",
    "            return self.debugged_solutions_cache[cache_key]\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", get_debugger_system_prompt(self.necessary_metadata)),\n",
    "            (\"user\",\n",
    "             f\"Given the reflections: '{reflections}', please refine the following SAS code:\\n```sas\\n{solution}\\n```\")\n",
    "        ])\n",
    "        try:\n",
    "            response = (prompt | self.llm | StrOutputParser()).invoke({\n",
    "                \"question\": question,\n",
    "                \"solution\": solution,\n",
    "                \"reflections\": reflections,\n",
    "                \"necessary_metadata\": self.necessary_metadata\n",
    "            })\n",
    "            refined_solution = self._extract_code(response)\n",
    "            logging.info(f\"Generated refined solution:\\n{refined_solution}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during refinement generation: {e}\")\n",
    "            refined_solution = \"\"\n",
    "\n",
    "        self.debugged_solutions_cache[cache_key] = refined_solution\n",
    "        return refined_solution\n",
    "\n",
    "    def _extract_code(self, response: str) -> str:\n",
    "        \"\"\"Extracts the SAS code from the LLM's response.\"\"\"\n",
    "        return response.strip()\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, llm, client, necessary_metadata, config: Dict):\n",
    "        self.llm = llm\n",
    "        self.client = client\n",
    "        self.necessary_metadata = necessary_metadata\n",
    "        self.evaluation_cache = {}\n",
    "        self.config = config\n",
    "\n",
    "    def evaluate_solution(self, question: str, solution: str, strategy: str) -> Tuple[str, float, str]:\n",
    "        \"\"\"Evaluates the generated solution, provides feedback, and assigns a numerical score.\"\"\"\n",
    "        logging.info(f\"Evaluating solution for question: {question}\")\n",
    "        cache_key = (question, solution, strategy)\n",
    "        if cache_key in self.evaluation_cache:\n",
    "            logging.info(f\"Using cached evaluation for question: {question}\")\n",
    "            return self.evaluation_cache[cache_key]\n",
    "            \n",
    "        if \"```sas\" in solution:\n",
    "            start_index = solution.find(\"```sas\") + 6  # +6 to skip \"```sas\"\n",
    "            end_index = solution.find(\"```\", start_index)\n",
    "        if end_index != -1:\n",
    "              solution = solution[start_index:end_index].strip()\n",
    "            \n",
    "        log = execute_sas_code(solution)\n",
    "        try:\n",
    "            error_reflection = self._get_error_reflection(question, solution, log)\n",
    "            requirement_reflection = self._get_requirement_reflection(question, solution, log)\n",
    "            strategy_adherence_reflection = self._get_strategy_adherence_reflection(question, solution, strategy)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during reflections generation : {e}\")\n",
    "            raise\n",
    "\n",
    "        # --- More Detailed Evaluation ---\n",
    "        evaluation = \"\"\n",
    "        critic_score = 0.0\n",
    "        feedback = \"\"\n",
    "\n",
    "        if error_reflection.error_count > 0:\n",
    "            feedback += \"Errors:\\n\"\n",
    "            feedback += f\"- {error_reflection.error_log}\\n\"\n",
    "            critic_score += self.config[\"error_weight\"]  # Use configurable weight\n",
    "            evaluation = \"Refine\"\n",
    "\n",
    "        if not requirement_reflection.requirements_met:\n",
    "            feedback += \"Missing Requirements:\\n\"\n",
    "            feedback += f\"- {requirement_reflection.missing_requirements}\\n\"\n",
    "            critic_score += self.config[\"requirement_weight\"]  # Use configurable weight\n",
    "            evaluation = \"Refine\" if evaluation != \"Refine\" else \"Refine\"\n",
    "\n",
    "        # Strategy Adherence Score\n",
    "        critic_score = (\n",
    "                            critic_score + strategy_adherence_reflection.adherence_score\n",
    "                        ) / 2 if evaluation == \"Refine\" else strategy_adherence_reflection.adherence_score\n",
    "\n",
    "        feedback += f\"Strategy Adherence: {strategy_adherence_reflection.adherence_score:.2f} - {strategy_adherence_reflection.reasoning}\\n\"\n",
    "\n",
    "        # --- Decision Logic ---\n",
    "        if error_reflection.error_count == 0 and requirement_reflection.requirements_met:\n",
    "            # Solution passes visible tests, perform verification\n",
    "            try:\n",
    "                if self.verify_solution(question, solution):\n",
    "                    evaluation = \"Accept\"\n",
    "                    critic_score = 1.0  # Perfect score if it passes verification\n",
    "                else:\n",
    "                    evaluation = \"Refine\"\n",
    "                    feedback += \"Solution passes visible tests but fails verification (potential overfitting or lack of robustness).\\n\"\n",
    "                    critic_score = self.config[\"verification_fail_score\"]  # Use configurable score\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during solution verification : {e}\")\n",
    "                evaluation = \"Refine\"\n",
    "                feedback += \"Solution passes visible tests but fails verification (potential overfitting or lack of robustness).\\n\"\n",
    "                critic_score = self.config[\"verification_fail_score\"]\n",
    "        elif critic_score < self.config[\"abort_threshold\"]:  # Use configurable threshold\n",
    "            evaluation = \"Abort\"\n",
    "\n",
    "        logging.info(f\"Evaluation: {evaluation}, Critic Score: {critic_score}, Feedback: {feedback}\")\n",
    "        self.evaluation_cache[cache_key] = (evaluation, critic_score, feedback)\n",
    "        return evaluation, critic_score, feedback\n",
    "\n",
    "    def verify_solution(self, question: str, solution: str) -> bool:\n",
    "        \"\"\"Verifies if a solution that passes visible tests is robust and generalizable.\"\"\"\n",
    "        logging.info(f\"Verifying solution for question: {question}\")\n",
    "        verification_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", get_critic_system_prompt(self.necessary_metadata)),\n",
    "            (\"user\", f\"\"\"\n",
    "        We have a SAS code solution that passes all visible test cases for the query: '{question}'.\n",
    "        ```sas\n",
    "        {solution}\n",
    "        ```\n",
    "        Your task is to assess whether this solution is likely to be correct for unseen test cases as well. \n",
    "        Consider the following:\n",
    "        - Does the code seem overly tailored to the specific visible test cases, or does it demonstrate a general understanding of the problem?\n",
    "        - Are there any potential edge cases or scenarios not covered by the visible tests that the code might fail on?\n",
    "\n",
    "        Answer with 'True' if the solution is likely to be correct for unseen test cases, and 'False' otherwise. Provide a brief explanation for your assessment.\n",
    "        \"\"\")\n",
    "    ])\n",
    "        try:\n",
    "            response = (verification_prompt | self.llm | StrOutputParser()).invoke({\n",
    "            \"question\": question,\n",
    "            \"solution\": solution,\n",
    "            \"necessary_metadata\": self.necessary_metadata\n",
    "        })\n",
    "\n",
    "            logging.info(f\"Raw verification response: {response}\")  # Log the raw response\n",
    "\n",
    "            # Normalize the response to handle variations (e.g., \"True.\", \"True \", \"TRUE\")\n",
    "            cleaned_response = response.strip().lower() \n",
    "\n",
    "            logging.info(f\"Cleaned verification response: {cleaned_response}\") # Log the cleaned response\n",
    "\n",
    "            if \"true\" in cleaned_response:\n",
    "                logging.info(\"Solution verification successful.\")\n",
    "                return True\n",
    "            else:\n",
    "                logging.warning(f\"Solution verification failed: {response}\")\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during solution verification: {e}\")\n",
    "             return False\n",
    "\n",
    "    def _get_error_reflection(self, query: str, sas_code: str, log: str) -> \"ErrorReflection\":\n",
    "        \"\"\"\n",
    "        Identifies errors in the SAS log and provides an ErrorReflection.\n",
    "        Now also tries to categorize the error.\n",
    "        \"\"\"\n",
    "        logging.info(\"Getting error reflection...\")\n",
    "        parser = PydanticOutputParser(pydantic_object=ErrorReflection)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a helpful assistant that identifies and categorizes errors in SAS code execution logs. {format_instructions}\"),\n",
    "            (\"user\", \"Query: {query}\\nCode:\\n```sas\\n{sas_code}\\n```\\nLog:\\n{log}\")\n",
    "        ])\n",
    "        try:\n",
    "            response = (prompt | self.llm | parser).invoke({\n",
    "                \"query\": query,\n",
    "                \"sas_code\": sas_code,\n",
    "                \"log\": log,\n",
    "                \"format_instructions\": parser.get_format_instructions()\n",
    "            })\n",
    "            logging.info(\n",
    "                f\"Error reflection: Error Count: {response.error_count}, Error Log: {response.error_log}, Error Category: {response.error_category}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during error reflection generation : {e}\")\n",
    "            raise\n",
    "\n",
    "    def _get_requirement_reflection(self, query: str, sas_code: str, log: str) -> \"RequirementReflection\":\n",
    "        \"\"\"\n",
    "        Determines whether the SAS code meets the requirements of the original query and provides a RequirementReflection.\n",
    "        Now uses a PydanticOutputParser for structured output.\n",
    "        \"\"\"\n",
    "        logging.info(\"Getting requirement reflection...\")\n",
    "        parser = PydanticOutputParser(pydantic_object=RequirementReflection)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"\n",
    "            You are a helpful assistant that identifies missing requirements in SAS code.\n",
    "            Use the metadata to understand the database and ensure the requirements are aligned with the database's structure.\n",
    "            {{format_instructions}}\n",
    "            metadata : {self.necessary_metadata}\"\"\"),\n",
    "            (\"user\", f\"\"\"\n",
    "            Original Query: {query}\n",
    "            Generated SAS Code:\n",
    "            ```sas\n",
    "            {sas_code}\n",
    "            ```\n",
    "            Does the SAS code fulfill all requirements stated in the original query?\n",
    "            Answer with 'True' if everything is generally satisfying to the average user, only give back \"False\" if there flagrant error of totally not understanding the query.\n",
    "            \"\"\")\n",
    "        ])\n",
    "        try:\n",
    "            response = (prompt | self.llm | parser).invoke({\n",
    "                \"query\": query,\n",
    "                \"sas_code\": sas_code,\n",
    "                \"necessary_metadata\": self.necessary_metadata,\n",
    "                \"format_instructions\": parser.get_format_instructions()\n",
    "            })\n",
    "            logging.info(\n",
    "                f\"Requirement reflection: Missing Requirements: {response.missing_requirements}, Requirements Met: {response.requirements_met}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during requirement reflection generation : {e}\")\n",
    "            raise\n",
    "\n",
    "    def _get_strategy_adherence_reflection(self, query: str, sas_code: str,\n",
    "                                            strategy: str) -> \"StrategyAdherenceReflection\":\n",
    "        \"\"\"\n",
    "        Evaluates the adherence of the SAS code to the given strategy and provides a StrategyAdherenceReflection.\n",
    "        Uses a PydanticOutputParser for structured output, including a numerical score.\n",
    "        \"\"\"\n",
    "        logging.info(\"Getting strategy adherence reflection...\")\n",
    "        parser = PydanticOutputParser(pydantic_object=StrategyAdherenceReflection)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"\n",
    "            You are a helpful assistant that evaluates the adherence of SAS code to a given strategy.\n",
    "            Use the metadata to understand the database and ensure the code aligns with the strategy and the database's structure.\n",
    "            {{format_instructions}}\n",
    "            metadata : {self.necessary_metadata}\n",
    "            \"\"\"),\n",
    "            (\"user\", f\"\"\"\n",
    "            Original Query: {query}\n",
    "            Strategy: {strategy}\n",
    "            Generated SAS Code:\n",
    "            ```sas\n",
    "            {sas_code}\n",
    "            ```\n",
    "            Assess the adherence of the SAS code to the given strategy. Consider:\n",
    "            1. How well does the code logically follow the strategy?\n",
    "            2. Does the code use appropriate data structures and algorithms as suggested by the strategy?\n",
    "            3. Are there any deviations from the strategy, and if so, are they justified?\n",
    "            Provide a numerical adherence score between 0 and 1, where 1 represents perfect adherence and 0 represents no adherence.\n",
    "            \"\"\")\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            response = (prompt | self.llm | parser).invoke({\n",
    "                \"query\": query,\n",
    "                \"sas_code\": sas_code,\n",
    "                \"strategy\": strategy,\n",
    "                \"necessary_metadata\": self.necessary_metadata,\n",
    "                \"format_instructions\": parser.get_format_instructions()\n",
    "            })\n",
    "            logging.info(\n",
    "                f\"Strategy adherence reflection: Adherence Score: {response.adherence_score}, Reasoning: {response.reasoning}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during strategy adherence reflection generation: {e}\")\n",
    "            raise\n",
    "\n",
    "class ErrorReflection(BaseModel):\n",
    "    error_count: int = Field(description=\"Number of errors in SAS log\")\n",
    "    error_log: str = Field(description=\"Portion of the log that contains error messages\")\n",
    "    error_category: str = Field(..., description=\"Category of the error (e.g., syntax, runtime, logical)\")\n",
    "\n",
    "class RequirementReflection(BaseModel):\n",
    "    missing_requirements: str = Field(description=\"Requirements not met in the code\")\n",
    "    requirements_met: bool = Field(description=\"Whether all requirements are met\")\n",
    "\n",
    "class StrategyAdherenceReflection(BaseModel):\n",
    "    adherence_score: float = Field(...,\n",
    "                                   description=\"Numerical score between 0 and 1 representing adherence to the strategy\")\n",
    "    reasoning: str = Field(..., description=\"Explanation of the adherence score, including any deviations from the strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45407faa-61a1-44e0-b188-cbb205f05a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Node and TreeState ---\n",
    "class Node:\n",
    "    def __init__(\n",
    "            self,\n",
    "            strategy: str,\n",
    "            solution: str,\n",
    "            evaluation: str,\n",
    "            config: Dict,\n",
    "            critic_score: float = 0,\n",
    "            parent: Optional[\"Node\"] = None,\n",
    "            question: Optional[str] = None,\n",
    "\n",
    "    ):\n",
    "        self.strategy = strategy\n",
    "        self.solution = solution\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.evaluation = evaluation\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "        self.critic_score = critic_score\n",
    "        self.score = 0\n",
    "        self.depth = parent.depth + 1 if parent is not None else 1\n",
    "        self.question = question if question is not None else (parent.question if parent is not None else None)\n",
    "        self.config = config\n",
    "        if self.evaluation == \"Accept\":\n",
    "            self.backpropagate(1, self.config)\n",
    "        else:\n",
    "            self.backpropagate(0, self.config)\n",
    "\n",
    "    def calculate_score(self, config: Dict) -> float:\n",
    "        \"\"\"Calculates the node's score based on execution results and Critic's evaluation.\"\"\"\n",
    "        logging.info(f\"Calculating score for node with strategy: {self.strategy}\")\n",
    "        execution_score = 0\n",
    "\n",
    "        if self.solution:  # Only calculate if a solution exists\n",
    "            log = execute_sas_code(self.solution)\n",
    "            num_tests = 0\n",
    "            passed_tests = 0\n",
    "\n",
    "            # Example: Simple pass/fail scoring (adapt based on your test case format)\n",
    "            for line in log.splitlines():\n",
    "                if \"passed:\" in line.lower():\n",
    "                    num_tests += 1\n",
    "                    if \"true\" in line.lower():\n",
    "                        passed_tests += 1\n",
    "                if \"test ok\" in line.lower():\n",
    "                    num_tests += 1\n",
    "                    passed_tests += 1\n",
    "\n",
    "            if num_tests > 0:\n",
    "                execution_score = passed_tests / num_tests\n",
    "\n",
    "        # Combine execution score and Critic's score (weighted average)\n",
    "        # Use weights from config\n",
    "        self.score = config[\"execution_weight\"] * execution_score + config[\"critic_weight\"] * self.critic_score\n",
    "        logging.info(f\"Node score calculated: {self.score}\")\n",
    "        return self.score\n",
    "\n",
    "    def upper_confidence_bound(self, exploration_weight: float = math.sqrt(2)) -> float:\n",
    "        if self.visits == 0:\n",
    "            return float('inf')\n",
    "        exploitation = self.value / self.visits\n",
    "        exploration = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "        return exploitation + exploration_weight * exploration\n",
    "\n",
    "    def backpropagate(self, reward: float, config: Dict) -> None:\n",
    "        \"\"\"Updates the node's value and propagates it to its ancestors.\"\"\"\n",
    "        logging.info(f\"Backpropagating reward: {reward} for node with strategy: {self.strategy}\")\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            if node.evaluation == \"Accept\":\n",
    "                node.score = 1\n",
    "            else:\n",
    "                node.calculate_score(config)  # Recalculate score during backpropagation\n",
    "            node = node.parent\n",
    "\n",
    "    def select_child(self) -> \"Node\":\n",
    "        \"\"\"Selects the best child using UCB.\"\"\"\n",
    "        logging.info(f\"Selecting child for node with strategy: {self.strategy}\")\n",
    "        best_child = None\n",
    "        best_ucb = float('-inf')\n",
    "        for child in self.children:\n",
    "            ucb = child.upper_confidence_bound()\n",
    "            if ucb > best_ucb:\n",
    "                best_ucb = ucb\n",
    "                best_child = child\n",
    "        logging.info(f\"Selected child with strategy: {best_child.strategy if best_child else 'None'}\")\n",
    "        return best_child\n",
    "\n",
    "    def is_terminal(self) -> bool:\n",
    "        return self.evaluation == \"Accept\"\n",
    "\n",
    "    def get_best_solution(self) -> \"Node\":\n",
    "        \"\"\"Retrieves the best solution from the subtree rooted at this node.\"\"\"\n",
    "        logging.info(f\"Getting best solution for node with strategy: {self.strategy}\")\n",
    "        all_nodes = [self] + self._get_all_children()\n",
    "        best_node = max(\n",
    "            all_nodes,\n",
    "            key=lambda node: (1 if node.evaluation == \"Accept\" else 0) * node.value\n",
    "        )\n",
    "        logging.info(f\"Best solution found: {best_node.solution}\")\n",
    "        return best_node\n",
    "\n",
    "    def _get_all_children(self) -> List[\"Node\"]:\n",
    "        \"\"\"Retrieves all children of this node (recursively).\"\"\"\n",
    "        all_nodes = []\n",
    "        queue = deque(self.children)\n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            all_nodes.append(node)\n",
    "            queue.extend(node.children)\n",
    "        return all_nodes\n",
    "\n",
    "class TreeState(TypedDict):\n",
    "    root: Optional[Node]\n",
    "    input: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc2255e5-cde3-4e9b-91d2-5da9a0aba354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tree Expansion Functions ---\n",
    "def generate_initial_strategies(state: TreeState, llm, client, necessary_metadata, config) -> dict:\n",
    "    \"\"\"Generates initial strategies and creates the root node.\"\"\"\n",
    "    question = state[\"input\"]\n",
    "    logging.info(f\"Generating initial strategies for question: {question}\")\n",
    "    thinker = Thinker(llm, necessary_metadata)\n",
    "    solver = Solver(llm, necessary_metadata)\n",
    "    critic = Critic(llm, client, necessary_metadata, config)\n",
    "\n",
    "    # Create a dummy root node\n",
    "    root = Node(strategy=\"Root\", solution=\"\", evaluation=\"Expand\", question=question, config=config)\n",
    "\n",
    "    # Generate and evaluate strategies one by one\n",
    "    strategies = []\n",
    "    for i in range(5):  # Assuming a maximum of 5 initial strategies as before\n",
    "        \n",
    "        if i == 0 :\n",
    "            # 1. Generate a strategy\n",
    "            strategy = thinker.generate_strategies(question, 1)\n",
    "            new_strategy = strategy[0]\n",
    "        else :\n",
    "            strategy = thinker.generate_strategies(question, 1, strategies)\n",
    "            new_strategy = strategy[0]\n",
    "            \n",
    "        if not new_strategy or new_strategy.lower() == \"no further strategies.\":\n",
    "            logging.info(\"No further strategies generated.\")\n",
    "            break\n",
    "\n",
    "        strategies.append(new_strategy)\n",
    "        logging.info(f\"Generated strategy {i+1}: {new_strategy}\")\n",
    "\n",
    "        # 2. Create a child node for the new strategy\n",
    "        child_node = Node(strategy=new_strategy, solution=\"\", evaluation=\"Expand\", parent=root, question=question,\n",
    "                          config=config)\n",
    "        root.children.append(child_node)\n",
    "\n",
    "        # 3. Generate a solution for the strategy\n",
    "        solution = solver.generate_solution(question, new_strategy)\n",
    "        child_node.solution = solution  # Update the node with the solution\n",
    "\n",
    "        # 4. Evaluate the solution\n",
    "        evaluation, critic_score, feedback = critic.evaluate_solution(question, solution, new_strategy)\n",
    "        child_node.evaluation = evaluation\n",
    "        child_node.critic_score = critic_score\n",
    "        child_node.calculate_score(config)\n",
    "\n",
    "        # 5. Check if the solution is verified (\"Accept\")\n",
    "        if evaluation == \"Accept\":\n",
    "            logging.info(f\"Verified solution found for strategy: {new_strategy}. Stopping initial strategy generation.\")\n",
    "            return {**state, \"root\": root}  # Stop early\n",
    "\n",
    "    logging.info(\"Finished generating initial strategies.\")\n",
    "    return {**state, \"root\": root}\n",
    "\n",
    "def expand_tree(state: TreeState, llm, client, necessary_metadata, config: Dict, max_depth: int = 5) -> dict:\n",
    "    \"\"\"Expands the tree based on node evaluations and scores.\"\"\"\n",
    "    root = state[\"root\"]\n",
    "    question = state[\"input\"]\n",
    "\n",
    "    # Check if an \"Accept\" node already exists before expanding\n",
    "    if any(node.evaluation == \"Accept\" for node in root._get_all_children()):\n",
    "        logging.info(\"Acceptable solution already exists. Skipping expansion.\")\n",
    "        return state\n",
    "\n",
    "    logging.info(f\"Expanding tree for question: {question}\")\n",
    "    current_node = select_node_to_expand(root)\n",
    "    logging.info(f\"Selected node for expansion: {current_node.strategy}\")\n",
    "\n",
    "    solver = Solver(llm, necessary_metadata)\n",
    "    critic = Critic(llm, client, necessary_metadata, config)\n",
    "    debugger = Debugger(llm, necessary_metadata)\n",
    "    thinker = Thinker(llm, necessary_metadata)\n",
    "\n",
    "    if current_node.evaluation == \"Expand\":\n",
    "        # Generate solution and evaluate\n",
    "        logging.info(f\"Generating solution for node: {current_node.strategy}\")\n",
    "        solution = solver.generate_solution(question, current_node.strategy)\n",
    "        logging.info(f\"Evaluating solution: {solution}\")\n",
    "        evaluation, critic_score, feedback = critic.evaluate_solution(question, solution, current_node.strategy)\n",
    "\n",
    "        # Update current node and create a new node\n",
    "        current_node.solution = solution\n",
    "        current_node.evaluation = evaluation\n",
    "        current_node.critic_score = critic_score\n",
    "        current_node.calculate_score(config)\n",
    "\n",
    "        new_node = Node(strategy=current_node.strategy, solution=solution, evaluation=evaluation,\n",
    "                        critic_score=critic_score, parent=current_node, question=question, config=config)\n",
    "        current_node.children.append(new_node)\n",
    "\n",
    "    elif current_node.evaluation == \"Refine\":\n",
    "        # Get reflections and refine solution\n",
    "        log = execute_sas_code(current_node.solution)\n",
    "        try:\n",
    "            error_reflection = critic._get_error_reflection(current_node.question, current_node.solution, log)\n",
    "            requirement_reflection = critic._get_requirement_reflection(current_node.question, current_node.solution,\n",
    "                                                                        log)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during reflections generation : {e}\")\n",
    "            raise\n",
    "\n",
    "        feedback = \"\"\n",
    "        if error_reflection.error_count > 0:\n",
    "            feedback += \"Errors:\\n\"\n",
    "            feedback += f\"- {error_reflection.error_log}\\n\"\n",
    "\n",
    "        if not requirement_reflection.requirements_met:\n",
    "            feedback += \"Missing Requirements:\\n\"\n",
    "            feedback += f\"- {requirement_reflection.missing_requirements}\\n\"\n",
    "\n",
    "        # Generate reflections using Thinker\n",
    "        logging.info(f\"Generating reflections for node: {current_node.strategy}\")\n",
    "        reflections = thinker.generate_reflections(current_node.question, current_node.solution, feedback, log,\n",
    "                                                   current_node.strategy)\n",
    "\n",
    "        if reflections == \"the code is good\":\n",
    "            # If no issues, simply append the current node again (no changes)\n",
    "            logging.info(f\"No issues found, appending current node again: {current_node.strategy}\")\n",
    "            evaluation, critic_score, critic_feedback = critic.evaluate_solution(\n",
    "                current_node.question, current_node.solution, current_node.strategy\n",
    "            )\n",
    "            new_node = Node(strategy=current_node.strategy, solution=current_node.solution, evaluation=evaluation,\n",
    "                            critic_score=critic_score, parent=current_node, question=question, config=config)\n",
    "            current_node.children.append(new_node)\n",
    "        else:\n",
    "            # Refine solution using Debugger\n",
    "            logging.info(f\"Refining solution for node: {current_node.strategy}\")\n",
    "            refined_solution = debugger.generate_refinement(question, current_node.solution, reflections)\n",
    "            logging.info(f\"Evaluating refined solution: {refined_solution}\")\n",
    "            evaluation, critic_score, feedback = critic.evaluate_solution(question, refined_solution,\n",
    "                                                                          current_node.strategy)\n",
    "\n",
    "            # Create new node for refined solution\n",
    "            new_node = Node(strategy=current_node.strategy, solution=refined_solution, evaluation=evaluation,\n",
    "                            critic_score=critic_score, parent=current_node, question=question, config=config)\n",
    "            current_node.children.append(new_node)\n",
    "\n",
    "            # Dynamic expansion based on Critic's score\n",
    "            if critic_score > config[\"expansion_threshold\"] and current_node.depth < max_depth:\n",
    "                logging.info(\n",
    "                    f\"Critic score {critic_score} above expansion threshold {config['expansion_threshold']}. Generating new strategies.\")\n",
    "                new_strategies = thinker.generate_strategies(question)\n",
    "                for new_strategy in new_strategies:\n",
    "                    new_node = Node(strategy=new_strategy, solution=\"\", evaluation=\"Expand\", parent=current_node,\n",
    "                                    question=question, config=config)\n",
    "                    current_node.children.append(new_node)\n",
    "            elif critic_score < config[\"abort_threshold\"]:\n",
    "                logging.info(\n",
    "                    f\"Critic score {critic_score} below abort threshold {config['abort_threshold']}. Marking node as Abort.\")\n",
    "                current_node.evaluation = \"Abort\"\n",
    "\n",
    "    return {**state, \"root\": root}\n",
    "\n",
    "def select_node_to_expand(root: Node) -> Node:\n",
    "    \"\"\"Selects a node to expand based on UCB and evaluation.\"\"\"\n",
    "    node = root\n",
    "    while node.children:\n",
    "        expandable_children = [child for child in node.children if child.evaluation == \"Expand\"]\n",
    "        if expandable_children:\n",
    "            logging.info(\"Selecting node to expand...\")\n",
    "            return max(expandable_children, key=lambda child: child.upper_confidence_bound())\n",
    "\n",
    "        refinable_children = [child for child in node.children if child.evaluation == \"Refine\"]\n",
    "        if refinable_children:\n",
    "            logging.info(\"Selecting node to refine...\")\n",
    "            return max(refinable_children, key=lambda child: child.upper_confidence_bound())\n",
    "\n",
    "        abortable_children = [child for child in node.children if child.evaluation == \"Abort\"]\n",
    "        if abortable_children:\n",
    "            logging.info(\"Found abortable node. Moving to parent.\")\n",
    "            if node == root:\n",
    "                return node\n",
    "            else:\n",
    "                node = node.parent\n",
    "        else:\n",
    "            node = node.children[0]\n",
    "\n",
    "    return node\n",
    "\n",
    "def should_continue(state: TreeState) -> str:\n",
    "    \"\"\"Determines whether the search should continue.\"\"\"\n",
    "    root = state[\"root\"]\n",
    "\n",
    "    # If a solution is found, stop\n",
    "    if any(node.evaluation == \"Accept\" for node in root._get_all_children()):\n",
    "        logging.info(\"Solution found. Stopping...\")\n",
    "        return END\n",
    "\n",
    "    # If any node is marked for expansion, continue\n",
    "    if any(node.evaluation == \"Expand\" for node in root._get_all_children()):\n",
    "        logging.info(\"Expansion required. Continuing...\")\n",
    "        return \"expand\"\n",
    "\n",
    "    # If any node is marked for expansion, continue\n",
    "    if any(node.evaluation == \"Refine\" for node in root._get_all_children()):\n",
    "        logging.info(\"Refinement required. Continuing...\")\n",
    "        return \"expand\"\n",
    "\n",
    "    # If max depth is reached for all nodes, stop\n",
    "    if all(node.depth >= 5 for node in root._get_all_children()):\n",
    "        logging.info(\"Max depth reached. Stopping...\")\n",
    "        return END\n",
    "\n",
    "    logging.info(\"No action determined. Continuing by default...\")\n",
    "    return \"expand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f68b40-23cd-42b7-86ad-1b2c4d3a3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize configuration (tune these parameters)\n",
    "    config = {\n",
    "        \"execution_weight\": 0.6,\n",
    "        \"critic_weight\": 0.4,\n",
    "        \"error_weight\": 0.2,\n",
    "        \"requirement_weight\": 0.3,\n",
    "        \"verification_fail_score\": 0.5,\n",
    "        \"abort_threshold\": 0.4,\n",
    "        \"expansion_threshold\": 0.6\n",
    "    }\n",
    "    # Example query\n",
    "    query = \"\"\"   \"\"\"\n",
    "\n",
    "    # Initialize LLM and necessary metadata\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", convert_system_message_to_human=True)\n",
    "    # Mocking the client for testing purposes, replace with actual client if needed\n",
    "    client = Client(\"Qwen/Qwen2.5-Coder-Artifacts\")\n",
    "    necessary_metadata = CachedMetadata.get_instance(metadata_dict, query)._metadata\n",
    "\n",
    "    # Initialize the tree state\n",
    "    state = {\"input\": query}\n",
    "    state = generate_initial_strategies(state, llm, necessary_metadata, config)\n",
    "\n",
    "    # Main loop\n",
    "    while should_continue(state) == \"expand\":\n",
    "        state = expand_tree(state, llm, client, necessary_metadata, config)\n",
    "\n",
    "    # Find the best solution\n",
    "    best_solution_node = state[\"root\"].get_best_solution()\n",
    "\n",
    "    if best_solution_node:\n",
    "        logging.info(f\"Best solution found:\\n{best_solution_node.solution}\")\n",
    "        if best_solution_node.evaluation == \"Accept\":\n",
    "            logging.info(\"Solution was accepted by the Critic.\")\n",
    "        else:\n",
    "            logging.info(\"Solution was not accepted, but it's the best found within the search limits.\")\n",
    "\n",
    "        # Extract data if needed\n",
    "        extract_data(best_solution_node.solution,client)\n",
    "    else:\n",
    "        logging.info(\"No solution found within the search limits.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
