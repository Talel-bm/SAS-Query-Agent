{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ad5ed-2173-4f38-a126-e782bf0a38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U --quiet langchain langgraph langchain_google_genai\n",
    "%pip install -U --quiet saspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb9a227-e221-4ba6-9ba2-eb12593744da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import os\n",
    "import getpass\n",
    "import re\n",
    "from collections import deque\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "import pandas as pd\n",
    "from gradio_client import Client\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import google.generativeai as genai\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "import saspy\n",
    "import json\n",
    "# Constants\n",
    "END = \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28030c1d-e4fc-4419-b9cf-3e373aa794b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436c7dfa-970a-4ee1-b099-71f97dc6b523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY ········\n"
     ]
    }
   ],
   "source": [
    "def _set_if_undefined(var: str) -> None:\n",
    "    \"\"\"Set environment variable if not already defined.\"\"\"\n",
    "    if os.environ.get(var):\n",
    "        return\n",
    "    os.environ[var] = getpass.getpass(var)\n",
    "\n",
    "_set_if_undefined(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "846d0b34-1a3a-42c1-a717-4ee5e174289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"\"\"%(asctime)s - %(levelname)s - %(message)s\n",
    "-------------------------------------------\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0118aa83-4531-4b17-b037-c0b1bfe05c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file_path = 'metadata.json'  # Assuming the file is in the same directory\n",
    "with open(metadata_file_path, 'r', encoding='utf-8') as f:\n",
    "    metadata_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f994b9-cb4e-416e-8dec-233e825c416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas = saspy.SASsession(cfgname='iomwin', cfgfile='sascfg.py')\n",
    "logging.info(\"SAS session initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5bab4d3-834d-4ee2-a3ff-fe6e7af12265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class DatabaseMetadata:\n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "\n",
    "class ColumnMetadata:\n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "\n",
    "class VerificationResult(BaseModel):\n",
    "    is_valid: bool\n",
    "    analysis: Dict[str, Any]\n",
    "    improvements: Dict[str, Any]\n",
    "    explanation: str\n",
    "    raw_response: Optional[str] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict[str, Any]) -> \"VerificationResult\":\n",
    "        return cls(\n",
    "            is_valid=data.get(\"is_valid\", False),\n",
    "            analysis=data.get(\"analysis\", {}),\n",
    "            improvements=data.get(\"improvements\", {}),\n",
    "            explanation=data.get(\"explanation\", \"\"),\n",
    "            raw_response=data.get(\"raw_response\")\n",
    "        )\n",
    "\n",
    "class ResponseSchemas:\n",
    "    \"\"\"Centralized response schemas configuration\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_db_schemas() -> List[ResponseSchema]:\n",
    "        return [\n",
    "            ResponseSchema(\n",
    "                name=\"databases\",\n",
    "                type=\"list\",\n",
    "                description=\"List of databases relevant to the query\",\n",
    "                properties=[\n",
    "                    ResponseSchema(name=\"database\", type=\"string\", description=\"Name of the database\"),\n",
    "                    ResponseSchema(name=\"reason\", type=\"string\", description=\"Reason for selecting this database\")\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_column_analysis_schemas() -> List[ResponseSchema]:\n",
    "        return [\n",
    "            ResponseSchema(name=\"key_entities\", type=\"list\", description=\"List of key entities being queried\"),\n",
    "            ResponseSchema(name=\"filter_conditions\", type=\"list\", description=\"List of filter conditions in the query\"),\n",
    "            ResponseSchema(name=\"required_calculations\", type=\"list\", description=\"List of required calculations\"),\n",
    "            ResponseSchema(name=\"related_to_description\", type=\"list\", description=\"Parts of the query related to the database description\")\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_column_selection_schemas() -> List[ResponseSchema]:\n",
    "        return [\n",
    "            ResponseSchema(name=\"columns\", type=\"list\", description=\"List of selected column names\")\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_verification_schemas() -> List[ResponseSchema]:\n",
    "        \"\"\"Schema for database verification results.\"\"\"\n",
    "        return [\n",
    "            ResponseSchema(name=\"is_valid\", type=\"boolean\", description=\"Whether the database selection is valid\"),\n",
    "            ResponseSchema(name=\"analysis\", type=\"object\", description=\"Analysis of the database selection\", properties=[\n",
    "                ResponseSchema(name=\"temporal_coverage\", type=\"string\", description=\"Analysis of temporal coverage\"),\n",
    "                ResponseSchema(name=\"processing_requirements\", type=\"string\", description=\"Analysis of processing requirements\"),\n",
    "                ResponseSchema(name=\"completeness\", type=\"string\", description=\"Analysis of database completeness\")\n",
    "            ]),\n",
    "            ResponseSchema(name=\"improvements\", type=\"object\", description=\"Suggested improvements to the database selection\", properties=[\n",
    "                ResponseSchema(name=\"databases_to_add\", type=\"object\", description=\"Databases to add with reasons\"),\n",
    "                ResponseSchema(name=\"databases_to_remove\", type=\"object\", description=\"Databases to remove with reasons\")\n",
    "            ]),\n",
    "            ResponseSchema(name=\"explanation\", type=\"string\", description=\"Explanation of the verification result\")\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_column_verification_schemas() -> List[ResponseSchema]:\n",
    "        \"\"\"Schema for column verification results.\"\"\"\n",
    "        return [\n",
    "            ResponseSchema(name=\"is_valid\", type=\"boolean\", description=\"Indicates if the column selection is valid\"),\n",
    "            ResponseSchema(name=\"analysis\", type=\"object\", description=\"Analysis of the column selection\", properties=[\n",
    "                ResponseSchema(name=\"mandatory_columns\", type=\"string\", description=\"Evaluation of mandatory column presence\"),\n",
    "                ResponseSchema(name=\"query_requirements\", type=\"string\", description=\"Verification of query requirement fulfillment\"),\n",
    "                ResponseSchema(name=\"rule_compliance\", type=\"string\", description=\"Assessment of adherence to specific rules\")\n",
    "            ]),\n",
    "            ResponseSchema(name=\"improvements\", type=\"object\", description=\"Suggested improvements for column selection\", properties=[\n",
    "                ResponseSchema(name=\"columns_to_add\", type=\"array\", description=\"List of columns to be added\"),\n",
    "                ResponseSchema(name=\"columns_to_remove\", type=\"array\", description=\"List of columns to be removed\")\n",
    "            ]),\n",
    "            ResponseSchema(name=\"explanation\", type=\"string\", description=\"Detailed explanation of the verification outcome\")\n",
    "        ]\n",
    "\n",
    "class PromptTemplates:\n",
    "    \"\"\"Centralized prompt template configuration\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_db_selection_template() -> str:\n",
    "        return \"\"\"\n",
    "You are a data analyst tasked with identifying the relevant databases to answer a specific query.\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "**Available Databases:**\n",
    "{database_descriptions}\n",
    "\n",
    "{previous_verification_feedback}\n",
    "\n",
    "**Instructions:**\n",
    "<redacted for privacy reasons>\n",
    "     \n",
    "8. **Response Format:**\n",
    "   - Respond strictly in a JSON dictionary: with the structure:  \n",
    "   {{\n",
    "       \"databases\" : [\n",
    "          {{\"database\": \"database_name1\", \"reason\": \"reason for selection\"}},\n",
    "          {{\"database\": \"database_name2\", \"reason\": \"reason for selection\"}},\n",
    "          ...\n",
    "       ]\n",
    "   }}\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_column_analysis_template() -> str:\n",
    "        return \"\"\"\n",
    "You are a data analyst. Analyze the following query to extract key information.\n",
    "\n",
    "**Question:** {question}\n",
    "**Database:** {db_name}\n",
    "**Reason for Selecting Database:** {reason}\n",
    "\n",
    "**Tasks:**\n",
    "1. Identify the **key entities** being queried (e.g., claims, policies, etc.).\n",
    "2. List the **filter conditions** mentioned in the query (e.g., date ranges, status types, specific codes).\n",
    "3. Determine the **required calculations** (e.g., reglements, reserves).\n",
    "4. Identify which parts of the query relate to the **database description**.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_column_selection_template() -> str:\n",
    "        return \"\"\"\n",
    "Based on the analysis below, select the relevant columns from the database.\n",
    "\n",
    "Analysis from Prompt 1:\n",
    "{analysis}\n",
    "\n",
    "Columns in {db_name}:\n",
    "{column_info}\n",
    "\n",
    "{previous_verification_feedback}\n",
    "\n",
    "**Specific Rules:**\n",
    "<redacted for privacy reasons>\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_verification_template() -> str:\n",
    "        \"\"\"Template for verifying database selections.\"\"\"\n",
    "        return \"\"\"\n",
    "    Verify the database selection for the following query:\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Current Selection:\n",
    "    {selected_dbs}\n",
    "\n",
    "    Available Databases:\n",
    "    {available_databases}\n",
    "\n",
    "    Verification Tasks:\n",
    "    1. Check temporal coverage:\n",
    "    - Are all required years covered?\n",
    "    - Are any unnecessary years included?\n",
    "    2. Verify processing requirements:\n",
    "    - Are databases for relevant processing periods included?\n",
    "    3. Assess database completeness:\n",
    "    - Are all necessary databases for the query included?\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_column_verification_template() -> str:\n",
    "        \"\"\"Template for verifying column selections within a database.\"\"\"\n",
    "        return \"\"\"\n",
    "    Verify the column selection for the following query and database:\n",
    "\n",
    "    Question: {question}\n",
    "    Database: {db_name}\n",
    "\n",
    "    Current Selection:\n",
    "    {selected_columns}\n",
    "\n",
    "    Available Columns:\n",
    "    {available_columns}\n",
    "\n",
    "    Verification Tasks:\n",
    "    <redacted for privacy reasons>\n",
    "\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_question_expansion_template() -> str:\n",
    "        return \"\"\"\n",
    "You are a helpful assistant tasked with clarifying and expanding user questions to make them more precise and easier to understand in the context of database metadata selection.\n",
    "\n",
    "**Original Question:** {question}\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Analyze the Question:** Carefully read the original question and identify any ambiguities, implicit assumptions, or missing information.\n",
    "2. **Expand and Clarify:** Rephrase the question to make it more explicit and precise.\n",
    "    - Explicitly state any assumptions that should be made (e.g., the current year is 2024).\n",
    "    - Resolve any ambiguities by using more specific terminology or adding context.\n",
    "    - Incorporate relevant domain knowledge or rules related to database selection.\n",
    "    - If specific databases or terms are mentioned, ensure they are accurately represented in the expanded question.\n",
    "3. **Consider the Following Rules:**\n",
    "    <redacted for privacy reasons>\n",
    "\n",
    "4. **Output Format:** Provide the expanded and clarified question as a single string.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "<redacted for privacy reasons>\n",
    "\n",
    "**Expanded Question:**\n",
    "\"\"\"\n",
    "\n",
    "class MetadataVerifier:\n",
    "    \"\"\"Handles verification of database and column selections\"\"\"\n",
    "\n",
    "    def __init__(self, model: ChatGoogleGenerativeAI):\n",
    "        self.model = model\n",
    "        self.verification_output_parser = StructuredOutputParser.from_response_schemas(\n",
    "            ResponseSchemas.get_verification_schemas()\n",
    "        )\n",
    "        self.verification_prompt = PromptTemplate(\n",
    "            template=PromptTemplates.get_verification_template(),\n",
    "            input_variables=[\"question\", \"selected_dbs\", \"available_databases\"],\n",
    "            partial_variables={\"format_instructions\": self.verification_output_parser.get_format_instructions()}\n",
    "        )\n",
    "        self.column_verification_output_parser = StructuredOutputParser.from_response_schemas(\n",
    "            ResponseSchemas.get_column_verification_schemas()\n",
    "        )\n",
    "        self.column_verification_prompt = PromptTemplate(\n",
    "            template=PromptTemplates.get_column_verification_template(),\n",
    "            input_variables=[\"question\", \"db_name\", \"selected_columns\", \"available_columns\"],\n",
    "            partial_variables={\"format_instructions\": self.column_verification_output_parser.get_format_instructions()}\n",
    "        )\n",
    "\n",
    "    def verify_databases(self, question: str, selected_dbs: Dict[str, str],\n",
    "                            metadata_dict: Dict[str, Any]) -> Tuple[bool, VerificationResult]:\n",
    "        \"\"\"Verifies the initial database selection using the LLM\"\"\"\n",
    "        try:\n",
    "            _input = self.verification_prompt.format_prompt(\n",
    "                question=question,\n",
    "                selected_dbs=json.dumps(selected_dbs, indent=2, ensure_ascii=False),\n",
    "                available_databases=json.dumps(\n",
    "                    {k: v['description'] for k, v in metadata_dict.items()},\n",
    "                    indent=2, ensure_ascii=False\n",
    "                )\n",
    "            )\n",
    "            response = self.model.invoke(_input.to_messages())\n",
    "            verification_result = VerificationResult.from_dict(\n",
    "                self.verification_output_parser.parse(response.content)\n",
    "            )\n",
    "            verification_result.raw_response = response.content\n",
    "\n",
    "            logging.info(f\"Database verification: {'VALID' if verification_result.is_valid else 'INVALID'}\")\n",
    "            return verification_result.is_valid, verification_result\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Database verification error: {e}\")\n",
    "            return False, VerificationResult(\n",
    "                is_valid=False,\n",
    "                analysis={},\n",
    "                improvements={},\n",
    "                explanation=f\"Verification failed: {str(e)}\",\n",
    "                raw_response=None\n",
    "            )\n",
    "\n",
    "    def verify_columns(self, question: str, db_name: str, selected_columns: List[str],\n",
    "                      db_metadata: Dict[str, Any]) -> Tuple[bool, VerificationResult]:\n",
    "        \"\"\"Verifies the column selection for a given database using the LLM\"\"\"\n",
    "        try:\n",
    "            _input = self.column_verification_prompt.format_prompt(\n",
    "                question=question,\n",
    "                db_name=db_name,\n",
    "                selected_columns=json.dumps(selected_columns, indent=2, ensure_ascii=False),\n",
    "                available_columns=json.dumps(db_metadata['columns'], indent=2, ensure_ascii=False)\n",
    "            )\n",
    "            response = self.model.invoke(_input.to_messages())\n",
    "\n",
    "            verification_result = VerificationResult.from_dict(\n",
    "                self.column_verification_output_parser.parse(response.content)\n",
    "            )\n",
    "            verification_result.raw_response = response.content\n",
    "\n",
    "            logging.info(f\"Column verification for {db_name}: {'VALID' if verification_result.is_valid else 'INVALID'}\")\n",
    "            return verification_result.is_valid, verification_result\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Column verification error: {e}\")\n",
    "            return False, VerificationResult(\n",
    "                is_valid=False,\n",
    "                analysis={},\n",
    "                improvements={},\n",
    "                explanation=f\"Verification failed: {str(e)}\",\n",
    "                raw_response=None\n",
    "            )\n",
    "            \n",
    "class MetadataSelector:\n",
    "    \"\"\"Main class for handling metadata selection process\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gemini-1.5-flash\", max_iterations: int = 5):\n",
    "        self.model = ChatGoogleGenerativeAI(\n",
    "            model=model_name,\n",
    "            temperature=0,\n",
    "            convert_system_message_to_human=True\n",
    "        )\n",
    "        self.verifier = MetadataVerifier(self.model)\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "        # Initialize parsers\n",
    "        self._init_parsers()\n",
    "\n",
    "        # Initialize prompts\n",
    "        self._init_prompts()\n",
    "\n",
    "    def _init_parsers(self):\n",
    "        \"\"\"Initialize all output parsers\"\"\"\n",
    "        self.db_output_parser = StructuredOutputParser.from_response_schemas(\n",
    "            ResponseSchemas.get_db_schemas()\n",
    "        )\n",
    "        self.column_analysis_parser = StructuredOutputParser.from_response_schemas(\n",
    "            ResponseSchemas.get_column_analysis_schemas()\n",
    "        )\n",
    "        self.column_selection_parser = StructuredOutputParser.from_response_schemas(\n",
    "            ResponseSchemas.get_column_selection_schemas()\n",
    "        )\n",
    "\n",
    "    def _init_prompts(self):\n",
    "        \"\"\"Initialize all prompt templates\"\"\"\n",
    "        self.db_prompt = PromptTemplate(\n",
    "            template=PromptTemplates.get_db_selection_template(),\n",
    "            input_variables=[\"question\", \"database_descriptions\"],\n",
    "            partial_variables={\"format_instructions\": self.db_output_parser.get_format_instructions()}\n",
    "        )\n",
    "        self.column_analysis_prompt = PromptTemplate(\n",
    "            template=PromptTemplates.get_column_analysis_template(),\n",
    "            input_variables=[\"question\", \"db_name\", \"reason\"],\n",
    "            partial_variables={\"format_instructions\": self.column_analysis_parser.get_format_instructions()}\n",
    "        )\n",
    "        self.column_selection_prompt = PromptTemplate(\n",
    "            template=PromptTemplates.get_column_selection_template(),\n",
    "            input_variables=[\"analysis\", \"db_name\", \"column_info\"],\n",
    "            partial_variables={\"format_instructions\": self.column_selection_parser.get_format_instructions()}\n",
    "        )\n",
    "\n",
    "    def _expand_question(self, question: str) -> str:\n",
    "        \"\"\"Expands the original question to make it clearer and more explicit.\"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            template=PromptTemplates.get_question_expansion_template(),\n",
    "            input_variables=[\"question\"],\n",
    "        )\n",
    "        _input = prompt.format_prompt(question=question)\n",
    "        response = self.model.invoke(_input.to_messages())\n",
    "        return response.content\n",
    "\n",
    "    def select_metadata(self, question: str, metadata_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Main method to select metadata in two stages\"\"\"\n",
    "        try:\n",
    "            # Stage 1: Database selection\n",
    "            selected_dbs = self._select_and_verify_databases(question, metadata_dict)\n",
    "            if not selected_dbs:\n",
    "                return {\"error\": \"Failed to select valid databases\"}\n",
    "\n",
    "            # Stage 2: Column selection\n",
    "            selected_columns = self._select_and_verify_columns(question, selected_dbs, metadata_dict)\n",
    "\n",
    "            return self._build_final_metadata(selected_dbs, selected_columns, metadata_dict)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Metadata selection failed: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _select_and_verify_databases(self, question: str, metadata_dict: Dict[str, Any]) -> Dict[str, str]:\n",
    "        \"\"\"Database selection with verification\"\"\"\n",
    "        iteration = 0\n",
    "        selected_dbs = {}\n",
    "        previous_verification = None\n",
    "\n",
    "        while iteration < self.max_iterations:\n",
    "            logging.info(f\"Database selection iteration {iteration + 1}\")\n",
    "\n",
    "            # Select databases using LLM\n",
    "            selected_dbs_with_reasons = self._select_databases(question, metadata_dict, previous_verification)\n",
    "            selected_dbs = {db: reason for db, reason in selected_dbs_with_reasons.items()}\n",
    "\n",
    "            if not selected_dbs:\n",
    "                iteration += 1\n",
    "                continue\n",
    "\n",
    "            # Verify selection\n",
    "            is_valid, feedback = self.verifier.verify_databases(question, selected_dbs, metadata_dict)\n",
    "            if is_valid:\n",
    "                logging.info(\"Database selection verified successfully\")\n",
    "                return selected_dbs\n",
    "\n",
    "            # Apply improvements\n",
    "            if isinstance(feedback, VerificationResult):\n",
    "                selected_dbs = self._refine_databases(selected_dbs, feedback.improvements)\n",
    "                previous_verification = feedback\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        logging.warning(\"Max iterations reached in database selection\")\n",
    "        return selected_dbs\n",
    "\n",
    "    def _select_databases(self, question: str, metadata_dict: Dict[str, Any],\n",
    "                          previous_verification: Optional[VerificationResult] = None) -> Dict[str, str]:\n",
    "        \"\"\"Select databases using LLM\"\"\"\n",
    "        try:\n",
    "            # Expand the question before using it for database selection\n",
    "            expanded_question = self._expand_question(question)\n",
    "\n",
    "            database_descriptions = self._format_database_descriptions(metadata_dict)\n",
    "\n",
    "            # Modify the prompt to include feedback if available\n",
    "            if previous_verification:\n",
    "                feedback_info = f\"\"\"\n",
    "Previous Verification Feedback:\n",
    "- Analysis: {previous_verification.analysis}\n",
    "- Explanation: {previous_verification.explanation}\n",
    "- Raw Response: {previous_verification.raw_response}\n",
    "\"\"\"\n",
    "            else:\n",
    "                feedback_info = \"\"\n",
    "\n",
    "            _input = self.db_prompt.format_prompt(\n",
    "                question=expanded_question,  # Use the expanded question here\n",
    "                database_descriptions=database_descriptions,\n",
    "                previous_verification_feedback=feedback_info\n",
    "            )\n",
    "            response = self.model.invoke(_input.to_messages())\n",
    "            return self._parse_database_selection(response)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Database selection error: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _select_and_verify_columns(\n",
    "        self,\n",
    "        question: str,\n",
    "        selected_dbs: Dict[str, str],\n",
    "        metadata_dict: Dict[str, Any]\n",
    "    ) -> Dict[str, List[str]]:\n",
    "        \"\"\"Column selection with verification\"\"\"\n",
    "        final_columns = {}\n",
    "\n",
    "        for db_name in selected_dbs:\n",
    "            if db_name not in metadata_dict:\n",
    "                logging.warning(f\"Database '{db_name}' not found in metadata_dict.\")\n",
    "                continue\n",
    "\n",
    "            iteration = 0\n",
    "            selected_cols = []\n",
    "            previous_verification = None\n",
    "\n",
    "            while iteration < self.max_iterations:\n",
    "                logging.info(f\"Column selection iteration {iteration + 1} for {db_name}\")\n",
    "\n",
    "                # Select columns using LLM\n",
    "                selected_cols = self._select_columns(\n",
    "                    question,\n",
    "                    db_name,\n",
    "                    selected_dbs[db_name],\n",
    "                    metadata_dict[db_name],\n",
    "                    previous_verification,\n",
    "                )\n",
    "\n",
    "                if not selected_cols:\n",
    "                    iteration += 1\n",
    "                    continue\n",
    "\n",
    "                # Verify selection\n",
    "                is_valid, feedback = self.verifier.verify_columns(\n",
    "                    question, db_name, selected_cols, metadata_dict[db_name]\n",
    "                )\n",
    "\n",
    "                if is_valid:\n",
    "                    logging.info(f\"Column selection verified successfully for {db_name}\")\n",
    "                    final_columns[db_name] = selected_cols\n",
    "                    break\n",
    "\n",
    "                # Apply improvements\n",
    "                if isinstance(feedback, VerificationResult):\n",
    "                    selected_cols = self._refine_columns(\n",
    "                        selected_cols, feedback.improvements\n",
    "                    )\n",
    "                    previous_verification = feedback\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "            if iteration >= self.max_iterations:\n",
    "                logging.warning(\n",
    "                    f\"Max iterations reached in column selection for {db_name}\"\n",
    "                )\n",
    "                final_columns[db_name] = selected_cols\n",
    "\n",
    "        return final_columns\n",
    "\n",
    "    def _select_columns(\n",
    "        self,\n",
    "        question: str,\n",
    "        db_name: str,\n",
    "        reason: str,\n",
    "        db_metadata: Dict[str, Any],\n",
    "        previous_verification: Optional[VerificationResult] = None,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Select columns using two-step process, optionally incorporating feedback from previous verification\"\"\"\n",
    "        try:\n",
    "            analysis = self._perform_column_analysis(question, db_name, reason)\n",
    "\n",
    "            # Modify the prompt to include feedback if available\n",
    "            if previous_verification:\n",
    "                feedback_info = (\n",
    "                    f\"Previous Verification Feedback:\\n\"\n",
    "                    f\"- Analysis: {previous_verification.analysis}\\n\"\n",
    "                    f\"- Explanation: {previous_verification.explanation}\\n\"\n",
    "                    f\"- Raw Response: {previous_verification.raw_response}\\n\"\n",
    "                )\n",
    "            else:\n",
    "                feedback_info = \"\"\n",
    "\n",
    "            return self._perform_column_selection(\n",
    "                analysis, db_name, db_metadata, feedback_info\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Column selection error for {db_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _format_database_descriptions(self, metadata_dict: Dict[str, Any]) -> str:\n",
    "        return \"\\n\".join(\n",
    "            f\"- {db_name}: {db_info['description']}\"\n",
    "            for db_name, db_info in metadata_dict.items()\n",
    "        )\n",
    "\n",
    "    def _get_llm_response(self, question: str, database_descriptions: str) -> str:\n",
    "        _input = self.db_prompt.format_prompt(\n",
    "            question=question, database_descriptions=database_descriptions\n",
    "        )\n",
    "        return self.model.invoke(_input.to_messages()).content\n",
    "\n",
    "    def _parse_database_selection(self, response: str) -> Dict[str, str]:\n",
    "        try:\n",
    "            parsed_output = self.db_output_parser.parse(response.content)\n",
    "            return {item[\"database\"]: item[\"reason\"] for item in parsed_output[\"databases\"]}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error parsing database selection response: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _perform_column_analysis(\n",
    "        self, question: str, db_name: str, reason: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        _input = self.column_analysis_prompt.format_prompt(\n",
    "            question=question, db_name=db_name, reason=reason\n",
    "        )\n",
    "        response = self.model.invoke(_input.to_messages())\n",
    "        return self.column_analysis_parser.parse(response.content)\n",
    "\n",
    "    def _perform_column_selection(\n",
    "        self, analysis: Dict[str, Any], db_name: str, db_metadata: Dict[str, Any], feedback_info: str = \"\"\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Perform column selection, optionally with feedback info\"\"\"\n",
    "        column_info = json.dumps(db_metadata[\"columns\"], indent=2)\n",
    "        _input = self.column_selection_prompt.format_prompt(\n",
    "            analysis=analysis,\n",
    "            db_name=db_name,\n",
    "            column_info=column_info,\n",
    "            previous_verification_feedback=feedback_info,\n",
    "        )\n",
    "        response = self.model.invoke(_input.to_messages())\n",
    "        parsed_output = self.column_selection_parser.parse(response.content)\n",
    "        return parsed_output[\"columns\"]\n",
    "\n",
    "    def _refine_databases(\n",
    "        self, current_dbs: Dict[str, str], improvements: Dict[str, Any]\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Refines database selection based on verification feedback\"\"\"\n",
    "        refined_dbs = current_dbs.copy()\n",
    "\n",
    "        if improvements is None:\n",
    "            return current_dbs\n",
    "\n",
    "        # Add new databases\n",
    "        to_add = improvements.get(\"databases_to_add\", {})\n",
    "        if isinstance(to_add, dict):\n",
    "            refined_dbs.update(to_add)\n",
    "\n",
    "        # Remove specified databases\n",
    "        to_remove = improvements.get(\"databases_to_remove\", {})\n",
    "        if isinstance(to_remove, dict):\n",
    "            for db in to_remove:\n",
    "                refined_dbs.pop(db, None)\n",
    "\n",
    "        return refined_dbs\n",
    "\n",
    "    def _refine_columns(\n",
    "        self, current_columns: List[str], improvements: Dict[str, Any]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Refines column selection based on verification feedback\"\"\"\n",
    "        refined_columns = current_columns.copy()\n",
    "\n",
    "        if improvements is None:\n",
    "            return current_columns\n",
    "\n",
    "        # Add new columns\n",
    "        to_add = improvements.get(\"columns_to_add\", [])\n",
    "        if isinstance(to_add, list):\n",
    "            refined_columns.extend(to_add)\n",
    "\n",
    "        # Remove specified columns\n",
    "        to_remove = improvements.get(\"columns_to_remove\", [])\n",
    "        if isinstance(to_remove, list):\n",
    "            refined_columns = [col for col in refined_columns if col not in to_remove]\n",
    "\n",
    "        return list(set(refined_columns))  # Remove duplicates\n",
    "\n",
    "    def _build_final_metadata(\n",
    "        self,\n",
    "        selected_dbs: Dict[str, str],\n",
    "        selected_columns: Dict[str, List[str]],\n",
    "        metadata_dict: Dict[str, Any],\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Builds final metadata dictionary\"\"\"\n",
    "        final_metadata = {}\n",
    "        for db_name in selected_dbs:\n",
    "            if db_name in metadata_dict:\n",
    "                db_metadata = {\n",
    "                    \"description\": metadata_dict[db_name][\"description\"],\n",
    "                    \"columns\": {},\n",
    "                }\n",
    "                if db_name in selected_columns:\n",
    "                    for col in selected_columns[db_name]:\n",
    "                        if col in metadata_dict[db_name][\"columns\"]:\n",
    "                            db_metadata[\"columns\"][col] = metadata_dict[db_name][\n",
    "                                \"columns\"\n",
    "                            ][col]\n",
    "                final_metadata[db_name] = db_metadata\n",
    "        return final_metadata\n",
    "\n",
    "def format_metadata_to_string(metadata_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"Formats the metadata dictionary into a human-readable string.\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    for db_name, db_info in metadata_dict.items():\n",
    "        formatted_string += f\"\\n{db_name}:\\n\"\n",
    "        if isinstance(db_info, dict):  # Check if db_info is a dictionary\n",
    "            formatted_string += f\"  Description: {db_info.get('description', 'N/A')}\\n\"\n",
    "            formatted_string += \"  Columns:\\n\"\n",
    "            for col_name, col_info in db_info.get('columns', {}).items():\n",
    "                formatted_string += f\"    - {col_name}: {col_info.get('description', 'N/A')}\\n\"\n",
    "        else:\n",
    "            formatted_string += f\"  Error: {db_info}\\n\"  # Handle the case where db_info is a string (error message)\n",
    "    return formatted_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2639e0ae-4368-482b-abd2-f96b31deec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "def execute_sas_code(sas_code: str) -> str:\n",
    "    \"\"\"Execute SAS code and return log.\"\"\"\n",
    "    logging.info(\"Executing SAS code...\")\n",
    "    try:\n",
    "        result = sas.submit(sas_code)\n",
    "        logging.info(\"SAS code executed. Returning log.\")\n",
    "        return result['LOG']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing SAS code: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "def extract_data(sas_code: str, client) -> None:\n",
    "    \"\"\"Execute SAS code and save results to Excel.\"\"\"\n",
    "    logging.info(\"Extracting data...\")\n",
    "    try:\n",
    "        sas.submit(sas_code)\n",
    "        # Assuming 'client' is defined globally or passed as a parameter\n",
    "        df_name_result = client.predict(\n",
    "            query=f\"Extract final dataframe name from, dont include 'WORK.', just the raw name:\\n{sas_code}\",\n",
    "            api_name=\"/generation_code\"\n",
    "        )\n",
    "\n",
    "        if df_name_result:\n",
    "            df_name = df_name_result[0]\n",
    "            df = sas.sd2df(df_name)\n",
    "            if not df.empty:\n",
    "                excel_file = f\"{df_name}.xlsx\"\n",
    "                df.to_excel(excel_file, index=False)\n",
    "                logging.info(f\"Data extracted to {excel_file}\")\n",
    "            else:\n",
    "                logging.warning(f\"Dataframe {df_name} is empty.\")\n",
    "        else:\n",
    "            logging.warning(\"Could not determine dataframe name for extraction.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6e6cb8f-162c-419b-8b41-d13734601a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt Templates and System Messages ---\n",
    "def get_thinker_system_prompt(necessary_metadata):\n",
    "    return f\"\"\"\n",
    "You are StarThinker, a strategic module of the StarData AI coding agent.\n",
    "You have a basic understanding of actuary line of thinking, especially concerning automobile insurance.\n",
    "Your primary role is to analyze a given data query and devise multiple, distinct strategies for answering it using SAS code. You do not generate code, only high-level strategies.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "- Given a data query and metadata, generate distinct and exhaustive strategies outlining how to approach the problem using the available data in the SAS databases.\n",
    "- Strategies should be concise (1-2 sentences each) and explore different logical paths to the solution.\n",
    "- Consider various ways to utilize the provided metadata, which includes database names, descriptions, column names, descriptions, types, and potential values.\n",
    "- Dynamically determine the appropriate number of strategies based on the complexity of the query. Stop generating new strategies if you deem them to be sufficient and exhaustive.\n",
    "- give your response in this format :\n",
    "    'strategy 1 : <details of this strategy>\\n\n",
    "    strategy 2 : <details of this strategy>\\n\n",
    "    ...'\n",
    "**Key Considerations from StarData:**\n",
    "\n",
    "<redacted for privacy reasons>\n",
    "\n",
    "**Metadata:**\n",
    "\n",
    "{necessary_metadata}\n",
    "\"\"\"\n",
    "\n",
    "def get_solver_system_prompt(necessary_metadata):\n",
    "    return f\"\"\"\n",
    "You are StarSolver, a code generation module of the StarData AI coding agent.\n",
    "You have a basic understanding of actuary line of thinking, especially concerning automobile insurance.\n",
    "You are an expert in SAS programming. Your task is to generate SAS code that accurately answers data queries based on provided strategies and metadata.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "- Given a data query and a specific strategy, generate a complete and executable SAS code solution.\n",
    "- The code should be syntactically correct and produce the desired output based on the query and strategy.\n",
    "- Output ONLY the SAS code, without any explanations or additional text.\n",
    "- Save all new dataframes in the WORK library (temporary).\n",
    "- When doing union joins, select only relevant columns.\n",
    "- Adhere to all coding instructions and guidelines specified below.\n",
    "\n",
    "**Coding Instructions from StarData:**\n",
    "\n",
    "<redacted for privacy reasons>\n",
    "\n",
    "**Metadata:**\n",
    "\n",
    "{necessary_metadata}\n",
    "\"\"\"\n",
    "\n",
    "def get_debugger_system_prompt(necessary_metadata):\n",
    "    return f\"\"\"\n",
    "You are StarDebugger, a code refinement module of the StarData AI coding agent.\n",
    "You have a basic understanding of actuary line of thinking, especially concerning automobile insurance.\n",
    "You are an expert in SAS programming and debugging. Your task is to refine and improve SAS code solutions based on provided feedback.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "- Given a SAS code solution, and feedback (which may include error messages, requirement shortcomings, or suggestions), generate a corrected and improved version of the code.\n",
    "- The refined code should address all issues mentioned in the feedback and produce the desired output according to the original query.\n",
    "- Output ONLY the refined SAS code, without any explanations or additional text.\n",
    "\n",
    "**Coding and Debugging Instructions from StarData:**\n",
    "<redacted for privacy reasons>\n",
    "\n",
    "**Metadata:**\n",
    "\n",
    "{necessary_metadata}\n",
    "\"\"\"\n",
    "\n",
    "def get_critic_system_prompt(necessary_metadata):\n",
    "    return f\"\"\"\n",
    "You are StarCritic, a solution evaluator module of the StarData AI coding agent.\n",
    "You have a basic understanding of actuary line of thinking, especially concerning automobile insurance.\n",
    "You are an expert in SAS programming and analysis. Your task is to evaluate SAS code solutions generated by the Solver agent, provide feedback, and assess their suitability for refinement or acceptance.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "- Given a SAS code solution, its corresponding strategy, and the original query, evaluate the solution's correctness, adherence to the strategy, and overall quality.\n",
    "- Provide a numerical score (critic_score) that reflects the solution's quality and potential for improvement.\n",
    "- Generate specific feedback (feedback) that identifies errors, missing requirements, and areas for improvement.\n",
    "- Determine whether the solution should be refined, aborted, or accepted based on your evaluation.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "\n",
    "- **Correctness:** Does the code execute without errors? Does it produce the expected output for visible test cases?\n",
    "- **Strategy Adherence:** Does the code effectively implement the given strategy? Does it logically follow the strategy and use appropriate data structures and algorithms?\n",
    "- **Robustness:** Is the solution likely to be correct for unseen test cases? Does it handle potential edge cases and demonstrate a general understanding of the problem?\n",
    "- **Requirements Fulfillment:** Does the code meet all the requirements stated in the original query? Are there any missing functionalities or discrepancies?\n",
    "\n",
    "**Decision Logic:**\n",
    "\n",
    "- **Refine:** If the solution has errors, fails to meet requirements, or does not fully adhere to the strategy, it should be refined.\n",
    "- **Abort:** If the solution has major flaws, scores very low on the evaluation criteria, or is unlikely to be improved with further refinement, it should be aborted.\n",
    "- **Accept:** If the solution passes all visible test cases, adheres to the strategy, meets all requirements, and is deemed robust, it should be accepted as the final solution.\n",
    "\n",
    "**Additional Instructions:**\n",
    "\n",
    "<redacted for privacy reasons>\n",
    "\n",
    "**Metadata:**\n",
    "\n",
    "{necessary_metadata}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ce92129-62ab-45d1-b9c9-d036775ff50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agent Classes ---\n",
    "class Thinker:\n",
    "    def __init__(self, llm, necessary_metadata):\n",
    "        self.llm = llm\n",
    "        self.necessary_metadata = necessary_metadata\n",
    "        self.strategies_cache: Dict[str, List[str]] = {}\n",
    "        self.reflections_cache: Dict[Tuple[str, str, str, str], List[str]] = {}\n",
    "\n",
    "    def generate_strategies(self, question: str, max_strategies: int = 5, previous_strategies: List[str] = None) -> List[str]:\n",
    "        \"\"\"Generates multiple strategies for solving the coding problem autoregressively.\"\"\"\n",
    "        logging.info(f\"Generating strategies for question: {question}\")\n",
    "        if question in self.strategies_cache:\n",
    "            logging.info(f\"Using cached strategies for question: {question}\")\n",
    "            return self.strategies_cache[question]\n",
    "\n",
    "        strategies = []\n",
    "        \n",
    "        if previous_strategies is not None:\n",
    "            strategies = previous_strategies\n",
    "\n",
    "        for i in range(max_strategies):\n",
    "            prompt = self._build_strategy_prompt(question, strategies)\n",
    "            try:\n",
    "                response = (prompt | self.llm | StrOutputParser()).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"necessary_metadata\": self.necessary_metadata,\n",
    "                    \"previous_strategies\": \"\\n\".join(strategies)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during strategy generation: {e}\")\n",
    "                break\n",
    "\n",
    "            new_strategy = self._extract_strategy(response)\n",
    "            if not new_strategy or new_strategy.lower() == \"no further strategies.\":\n",
    "                logging.info(\"No further strategies generated.\")\n",
    "                break\n",
    "\n",
    "            strategies.append(new_strategy)\n",
    "            logging.info(f\"Generated strategy {i+1}: {new_strategy}\")\n",
    "\n",
    "        self.strategies_cache[question] = strategies\n",
    "        return strategies\n",
    "\n",
    "    def _build_strategy_prompt(self, question: str, previous_strategies: List[str]) -> ChatPromptTemplate:\n",
    "        \"\"\"Builds the prompt for generating the next strategy.\"\"\"\n",
    "        system_prompt = get_thinker_system_prompt(self.necessary_metadata)\n",
    "\n",
    "        if previous_strategies:\n",
    "            system_prompt += \"\\n\\nPrevious Strategies:\\n\" + \"\\n\".join(\n",
    "                [f\"{i+1}. {strategy}\" for i, strategy in enumerate(previous_strategies)]\n",
    "            )\n",
    "        system_prompt += \"\\n\\nGenerate the next strategy, or write 'No further strategies.' if no further strategies can be generated.\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", f\"Question: {question}\\nStrategy:\")\n",
    "        ])\n",
    "        return prompt\n",
    "\n",
    "    def _extract_strategy(self, response: str) -> str:\n",
    "        \"\"\"Extracts a strategy from the LLM's response.\"\"\"\n",
    "        # Use regex to find the strategy after 'strategy x :'\n",
    "        match = re.search(r\"strategy \\d+ : (.*)\", response, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def generate_reflections(self, question: str, solution: str, feedback: str, log: str, strategy: str,\n",
    "                             num_reflections: int = 3) -> List[str]:\n",
    "        \"\"\"Generates reflections on the code autoregressively.\"\"\"\n",
    "        logging.info(f\"Generating reflections for question: {question}\")\n",
    "        cache_key = (question, solution, feedback, log)\n",
    "        if cache_key in self.reflections_cache:\n",
    "            logging.info(f\"Using cached reflections for question: {question}\")\n",
    "            return self.reflections_cache[cache_key]\n",
    "\n",
    "        reflections = []\n",
    "        for _ in range(num_reflections):\n",
    "            prompt = self._build_reflection_prompt(question, solution, feedback, log, strategy, reflections)\n",
    "            try:\n",
    "                response = (prompt | self.llm | StrOutputParser()).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"solution\": solution,\n",
    "                    \"feedback\": feedback,\n",
    "                    \"log\": log,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"necessary_metadata\": self.necessary_metadata,\n",
    "                    \"previous_reflections\": \"\\n\".join(reflections)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during reflection generation: {e}\")\n",
    "                break\n",
    "\n",
    "            new_reflection = response.strip()\n",
    "            if not new_reflection or new_reflection.lower() == \"no further reflections.\":\n",
    "                logging.info(\"No further reflections generated.\")\n",
    "                break\n",
    "            reflections.append(new_reflection)\n",
    "            logging.info(f\"Generated reflection: {new_reflection}\")\n",
    "\n",
    "        self.reflections_cache[cache_key] = reflections\n",
    "        return reflections\n",
    "\n",
    "    def _build_reflection_prompt(self, question: str, solution: str, feedback: str, log: str, strategy: str,\n",
    "                                 previous_reflections: List[str]) -> ChatPromptTemplate:\n",
    "        \"\"\"Builds the prompt for generating the next reflection.\"\"\"\n",
    "        system_prompt = get_thinker_system_prompt(self.necessary_metadata)\n",
    "        system_prompt += f\"\\n\\nStrategy used: {strategy}\"\n",
    "        system_prompt += \"\\n\\nYour task is to generate reflections on the following SAS code solution, considering the feedback and log provided. \"\n",
    "        system_prompt += \"Reflections should be concise and focus on identifying areas for improvement or issues in the code.\"\n",
    "\n",
    "        if previous_reflections:\n",
    "            system_prompt += \"\\n\\nPrevious Reflections:\\n\" + \"\\n\".join(\n",
    "                [f\"{i+1}. {reflection}\" for i, reflection in enumerate(previous_reflections)]\n",
    "            )\n",
    "            system_prompt += \"\\n\\nGenerate the next reflection, or write 'No further reflections.' if no further reflections are needed\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", f\"Given the question: '{question}', the strategy: '{strategy}', the following SAS code solution: \\n\"\n",
    "                     f\"```sas\\n{solution}\\n```\\n\"\n",
    "                     f\"and the feedback from the Critic: '{feedback}',\\n\"\n",
    "                     f\"and the log from the code execution : '{log}', \\n\"\n",
    "                     f\"please generate reflections to guide the code refinement process.\")\n",
    "        ])\n",
    "        return prompt\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self, llm, necessary_metadata):\n",
    "        self.llm = llm\n",
    "        self.necessary_metadata = necessary_metadata\n",
    "        self.solutions_cache: Dict[Tuple[str, str], str] = {}\n",
    "\n",
    "    def generate_solution(self, question: str, strategy: str) -> str:\n",
    "        \"\"\"Generates an initial SAS code solution based on the given strategy.\"\"\"\n",
    "        logging.info(f\"Generating solution for question: {question} with strategy: {strategy}\")\n",
    "        cache_key = (question, strategy)\n",
    "        if cache_key in self.solutions_cache:\n",
    "            logging.info(f\"Using cached solution for question: {question} and strategy: {strategy}\")\n",
    "            return self.solutions_cache[cache_key]\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", get_solver_system_prompt(self.necessary_metadata)),\n",
    "            (\"user\", f\"Given the strategy: '{strategy}', please write SAS code to answer this question: {question}\")\n",
    "        ])\n",
    "        try:\n",
    "            response = (prompt | self.llm | StrOutputParser()).invoke({\n",
    "                \"question\": question,\n",
    "                \"strategy\": strategy,\n",
    "                \"necessary_metadata\": self.necessary_metadata\n",
    "            })\n",
    "            solution = self._extract_code(response)\n",
    "            logging.info(f\"Generated solution:\\n{solution}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during solution generation: {e}\")\n",
    "            solution = \"\"\n",
    "\n",
    "        self.solutions_cache[cache_key] = solution\n",
    "        return solution\n",
    "\n",
    "    def _extract_code(self, response: str) -> str:\n",
    "        \"\"\"Extracts the SAS code from the LLM's response.\"\"\"\n",
    "        return response.strip()\n",
    "\n",
    "class Debugger:\n",
    "    def __init__(self, llm, necessary_metadata):\n",
    "        self.llm = llm\n",
    "        self.necessary_metadata = necessary_metadata\n",
    "        self.debugged_solutions_cache: Dict[Tuple[str, str, Tuple[str]], str] = {}  # Update type hint\n",
    "\n",
    "    def generate_refinement(self, question: str, solution: str, reflections: list) -> str:\n",
    "        \"\"\"Refines the given SAS code solution based on reflections.\"\"\"\n",
    "        logging.info(f\"Generating refinement for question: {question}\")\n",
    "        cache_key = (question, solution, tuple(reflections))  # Convert to tuple\n",
    "        if cache_key in self.debugged_solutions_cache:\n",
    "            logging.info(f\"Using cached refined solution for question: {question}\")\n",
    "            return self.debugged_solutions_cache[cache_key]\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", get_debugger_system_prompt(self.necessary_metadata)),\n",
    "            (\"user\",\n",
    "             f\"Given the reflections: '{reflections}', please refine the following SAS code:\\n```sas\\n{solution}\\n```\")\n",
    "        ])\n",
    "        try:\n",
    "            response = (prompt | self.llm | StrOutputParser()).invoke({\n",
    "                \"question\": question,\n",
    "                \"solution\": solution,\n",
    "                \"reflections\": reflections,\n",
    "                \"necessary_metadata\": self.necessary_metadata\n",
    "            })\n",
    "            refined_solution = self._extract_code(response)\n",
    "            logging.info(f\"Generated refined solution:\\n{refined_solution}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during refinement generation: {e}\")\n",
    "            refined_solution = \"\"\n",
    "\n",
    "        self.debugged_solutions_cache[cache_key] = refined_solution\n",
    "        return refined_solution\n",
    "\n",
    "    def _extract_code(self, response: str) -> str:\n",
    "        \"\"\"Extracts the SAS code from the LLM's response.\"\"\"\n",
    "        return response.strip()\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, llm, client, necessary_metadata, config: Dict):\n",
    "        self.llm = llm\n",
    "        self.client = client\n",
    "        self.necessary_metadata = necessary_metadata\n",
    "        self.evaluation_cache = {}\n",
    "        self.config = config\n",
    "\n",
    "    def evaluate_solution(self, question: str, solution: str, strategy: str) -> Tuple[str, float, str]:\n",
    "        \"\"\"Evaluates the generated solution, provides feedback, and assigns a numerical score.\"\"\"\n",
    "        logging.info(f\"Evaluating solution for question: {question}\")\n",
    "        cache_key = (question, solution, strategy)\n",
    "        if cache_key in self.evaluation_cache:\n",
    "            logging.info(f\"Using cached evaluation for question: {question}\")\n",
    "            return self.evaluation_cache[cache_key]\n",
    "            \n",
    "        if \"```sas\" in solution:\n",
    "            start_index = solution.find(\"```sas\") + 6  # +6 to skip \"```sas\"\n",
    "            end_index = solution.find(\"```\", start_index)\n",
    "            if end_index != -1:\n",
    "              solution = solution[start_index:end_index].strip()\n",
    "            \n",
    "        log = execute_sas_code(solution)\n",
    "        try:\n",
    "            error_reflection = self._get_error_reflection(question, solution, log)\n",
    "            requirement_reflection = self._get_requirement_reflection(question, solution, log)\n",
    "            strategy_adherence_reflection = self._get_strategy_adherence_reflection(question, solution, strategy)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during reflections generation : {e}\")\n",
    "            raise\n",
    "\n",
    "        # --- More Detailed Evaluation ---\n",
    "        evaluation = \"\"\n",
    "        critic_score = 0.0\n",
    "        feedback = \"\"\n",
    "\n",
    "        if error_reflection.error_count > 0:\n",
    "            feedback += \"Errors:\\n\"\n",
    "            feedback += f\"- {error_reflection.error_log}\\n\"\n",
    "            critic_score += self.config[\"error_weight\"]  # Use configurable weight\n",
    "            evaluation = \"Refine\"\n",
    "\n",
    "        if not requirement_reflection.requirements_met:\n",
    "            feedback += \"Missing Requirements:\\n\"\n",
    "            feedback += f\"- {requirement_reflection.missing_requirements}\\n\"\n",
    "            critic_score += self.config[\"requirement_weight\"]  # Use configurable weight\n",
    "            evaluation = \"Refine\" if evaluation != \"Refine\" else \"Refine\"\n",
    "\n",
    "        # Strategy Adherence Score\n",
    "        critic_score = (\n",
    "                            critic_score + strategy_adherence_reflection.adherence_score\n",
    "                        ) / 2 if evaluation == \"Refine\" else strategy_adherence_reflection.adherence_score\n",
    "\n",
    "        feedback += f\"Strategy Adherence: {strategy_adherence_reflection.adherence_score:.2f} - {strategy_adherence_reflection.reasoning}\\n\"\n",
    "\n",
    "        # --- Decision Logic ---\n",
    "        if error_reflection.error_count == 0 and requirement_reflection.requirements_met:\n",
    "            # Solution passes visible tests, perform verification\n",
    "            try:\n",
    "                if self.verify_solution(question, solution):\n",
    "                    evaluation = \"Accept\"\n",
    "                    critic_score = 1.0  # Perfect score if it passes verification\n",
    "                else:\n",
    "                    evaluation = \"Refine\"\n",
    "                    feedback += \"Solution passes visible tests but fails verification (potential overfitting or lack of robustness).\\n\"\n",
    "                    critic_score = self.config[\"verification_fail_score\"]  # Use configurable score\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during solution verification : {e}\")\n",
    "                evaluation = \"Refine\"\n",
    "                feedback += \"Solution passes visible tests but fails verification (potential overfitting or lack of robustness).\\n\"\n",
    "                critic_score = self.config[\"verification_fail_score\"]\n",
    "        elif critic_score < self.config[\"abort_threshold\"]:  # Use configurable threshold\n",
    "            evaluation = \"Abort\"\n",
    "\n",
    "        logging.info(f\"Evaluation: {evaluation}, Critic Score: {critic_score}, Feedback: {feedback}\")\n",
    "        self.evaluation_cache[cache_key] = (evaluation, critic_score, feedback)\n",
    "        return evaluation, critic_score, feedback\n",
    "\n",
    "    def verify_solution(self, question: str, solution: str) -> bool:\n",
    "        \"\"\"Verifies if a solution that passes visible tests is robust and generalizable.\"\"\"\n",
    "        logging.info(f\"Verifying solution for question: {question}\")\n",
    "        verification_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", get_critic_system_prompt(self.necessary_metadata)),\n",
    "            (\"user\", f\"\"\"\n",
    "        We have a SAS code solution that passes all visible test cases for the query: '{question}'.\n",
    "        ```sas\n",
    "        {solution}\n",
    "        ```\n",
    "        Your task is to assess whether this solution is likely to be correct for unseen test cases as well. \n",
    "        Consider the following:\n",
    "        - Does the code seem overly tailored to the specific visible test cases, or does it demonstrate a general understanding of the problem?\n",
    "        - Are there any potential edge cases or scenarios not covered by the visible tests that the code might fail on?\n",
    "\n",
    "        Answer with 'True' if the solution is likely to be correct for unseen test cases, and 'False' otherwise. Provide a brief explanation for your assessment.\n",
    "        \"\"\")\n",
    "    ])\n",
    "        try:\n",
    "            response = (verification_prompt | self.llm | StrOutputParser()).invoke({\n",
    "            \"question\": question,\n",
    "            \"solution\": solution,\n",
    "            \"necessary_metadata\": self.necessary_metadata\n",
    "        })\n",
    "\n",
    "            logging.info(f\"Raw verification response: {response}\")  # Log the raw response\n",
    "\n",
    "            # Normalize the response to handle variations (e.g., \"True.\", \"True \", \"TRUE\")\n",
    "            cleaned_response = response.strip().lower() \n",
    "\n",
    "            logging.info(f\"Cleaned verification response: {cleaned_response}\") # Log the cleaned response\n",
    "\n",
    "            if \"true\" in cleaned_response:\n",
    "                logging.info(\"Solution verification successful.\")\n",
    "                return True\n",
    "            else:\n",
    "                logging.warning(f\"Solution verification failed: {response}\")\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during solution verification: {e}\")\n",
    "             return False\n",
    "\n",
    "    def _get_error_reflection(self, query: str, sas_code: str, log: str) -> \"ErrorReflection\":\n",
    "        \"\"\"\n",
    "        Identifies errors in the SAS log and provides an ErrorReflection.\n",
    "        Now also tries to categorize the error.\n",
    "        \"\"\"\n",
    "        logging.info(\"Getting error reflection...\")\n",
    "        parser = PydanticOutputParser(pydantic_object=ErrorReflection)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a helpful assistant that identifies and categorizes errors in SAS code execution logs. {format_instructions}\"),\n",
    "            (\"user\", \"Query: {query}\\nCode:\\n```sas\\n{sas_code}\\n```\\nLog:\\n{log}\")\n",
    "        ])\n",
    "        try:\n",
    "            response = (prompt | self.llm | parser).invoke({\n",
    "                \"query\": query,\n",
    "                \"sas_code\": sas_code,\n",
    "                \"log\": log,\n",
    "                \"format_instructions\": parser.get_format_instructions()\n",
    "            })\n",
    "            logging.info(\n",
    "                f\"Error reflection: Error Count: {response.error_count}, Error Log: {response.error_log}, Error Category: {response.error_category}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during error reflection generation : {e}\")\n",
    "            raise\n",
    "\n",
    "    def _get_requirement_reflection(self, query: str, sas_code: str, log: str) -> \"RequirementReflection\":\n",
    "        \"\"\"\n",
    "        Determines whether the SAS code meets the requirements of the original query and provides a RequirementReflection.\n",
    "        Now uses a PydanticOutputParser for structured output.\n",
    "        \"\"\"\n",
    "        logging.info(\"Getting requirement reflection...\")\n",
    "        parser = PydanticOutputParser(pydantic_object=RequirementReflection)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"\n",
    "            You are a helpful assistant that identifies missing requirements in SAS code.\n",
    "            Use the metadata to understand the database and ensure the requirements are aligned with the database's structure.\n",
    "            {{format_instructions}}\n",
    "            metadata : {self.necessary_metadata}\"\"\"),\n",
    "            (\"user\", f\"\"\"\n",
    "            Original Query: {query}\n",
    "            Generated SAS Code:\n",
    "            ```sas\n",
    "            {sas_code}\n",
    "            ```\n",
    "            Does the SAS code fulfill all requirements stated in the original query?\n",
    "            Answer with 'True' if everything is generally satisfying to the average user, only give back \"False\" if there flagrant error of totally not understanding the query.\n",
    "            \"\"\")\n",
    "        ])\n",
    "        try:\n",
    "            response = (prompt | self.llm | parser).invoke({\n",
    "                \"query\": query,\n",
    "                \"sas_code\": sas_code,\n",
    "                \"necessary_metadata\": self.necessary_metadata,\n",
    "                \"format_instructions\": parser.get_format_instructions()\n",
    "            })\n",
    "            logging.info(\n",
    "                f\"Requirement reflection: Missing Requirements: {response.missing_requirements}, Requirements Met: {response.requirements_met}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during requirement reflection generation : {e}\")\n",
    "            raise\n",
    "\n",
    "    def _get_strategy_adherence_reflection(self, query: str, sas_code: str,\n",
    "                                            strategy: str) -> \"StrategyAdherenceReflection\":\n",
    "        \"\"\"\n",
    "        Evaluates the adherence of the SAS code to the given strategy and provides a StrategyAdherenceReflection.\n",
    "        Uses a PydanticOutputParser for structured output, including a numerical score.\n",
    "        \"\"\"\n",
    "        logging.info(\"Getting strategy adherence reflection...\")\n",
    "        parser = PydanticOutputParser(pydantic_object=StrategyAdherenceReflection)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"\n",
    "            You are a helpful assistant that evaluates the adherence of SAS code to a given strategy.\n",
    "            Use the metadata to understand the database and ensure the code aligns with the strategy and the database's structure.\n",
    "            {{format_instructions}}\n",
    "            metadata : {self.necessary_metadata}\n",
    "            \"\"\"),\n",
    "            (\"user\", f\"\"\"\n",
    "            Original Query: {query}\n",
    "            Strategy: {strategy}\n",
    "            Generated SAS Code:\n",
    "            ```sas\n",
    "            {sas_code}\n",
    "            ```\n",
    "            Assess the adherence of the SAS code to the given strategy. Consider:\n",
    "            1. How well does the code logically follow the strategy?\n",
    "            2. Does the code use appropriate data structures and algorithms as suggested by the strategy?\n",
    "            3. Are there any deviations from the strategy, and if so, are they justified?\n",
    "            Provide a numerical adherence score between 0 and 1, where 1 represents perfect adherence and 0 represents no adherence.\n",
    "            \"\"\")\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            response = (prompt | self.llm | parser).invoke({\n",
    "                \"query\": query,\n",
    "                \"sas_code\": sas_code,\n",
    "                \"strategy\": strategy,\n",
    "                \"necessary_metadata\": self.necessary_metadata,\n",
    "                \"format_instructions\": parser.get_format_instructions()\n",
    "            })\n",
    "            logging.info(\n",
    "                f\"Strategy adherence reflection: Adherence Score: {response.adherence_score}, Reasoning: {response.reasoning}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during strategy adherence reflection generation: {e}\")\n",
    "            raise\n",
    "\n",
    "class ErrorReflection(BaseModel):\n",
    "    error_count: int = Field(description=\"Number of errors in SAS log\")\n",
    "    error_log: str = Field(description=\"Portion of the log that contains error messages\")\n",
    "    error_category: str = Field(..., description=\"Category of the error (e.g., syntax, runtime, logical)\")\n",
    "\n",
    "class RequirementReflection(BaseModel):\n",
    "    missing_requirements: str = Field(description=\"Requirements not met in the code\")\n",
    "    requirements_met: bool = Field(description=\"Whether all requirements are met\")\n",
    "\n",
    "class StrategyAdherenceReflection(BaseModel):\n",
    "    adherence_score: float = Field(...,\n",
    "                                   description=\"Numerical score between 0 and 1 representing adherence to the strategy\")\n",
    "    reasoning: str = Field(..., description=\"Explanation of the adherence score, including any deviations from the strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45407faa-61a1-44e0-b188-cbb205f05a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Node and TreeState ---\n",
    "class Node:\n",
    "    def __init__(\n",
    "            self,\n",
    "            strategy: str,\n",
    "            solution: str,\n",
    "            evaluation: str,\n",
    "            config: Dict,\n",
    "            critic_score: float = 0,\n",
    "            parent: Optional[\"Node\"] = None,\n",
    "            question: Optional[str] = None,\n",
    "\n",
    "    ):\n",
    "        self.strategy = strategy\n",
    "        self.solution = solution\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.evaluation = evaluation\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "        self.critic_score = critic_score\n",
    "        self.score = 0\n",
    "        self.depth = parent.depth + 1 if parent is not None else 1\n",
    "        self.question = question if question is not None else (parent.question if parent is not None else None)\n",
    "        self.config = config\n",
    "        if self.evaluation == \"Accept\":\n",
    "            self.backpropagate(1, self.config)\n",
    "        else:\n",
    "            self.backpropagate(0, self.config)\n",
    "\n",
    "    def calculate_score(self, config: Dict) -> float:\n",
    "        \"\"\"Calculates the node's score based on execution results and Critic's evaluation.\"\"\"\n",
    "        logging.info(f\"Calculating score for node with strategy: {self.strategy}\")\n",
    "        execution_score = 0\n",
    "\n",
    "        if self.solution:  # Only calculate if a solution exists\n",
    "            log = execute_sas_code(self.solution)\n",
    "            num_tests = 0\n",
    "            passed_tests = 0\n",
    "\n",
    "            # Example: Simple pass/fail scoring (adapt based on your test case format)\n",
    "            for line in log.splitlines():\n",
    "                if \"passed:\" in line.lower():\n",
    "                    num_tests += 1\n",
    "                    if \"true\" in line.lower():\n",
    "                        passed_tests += 1\n",
    "                if \"test ok\" in line.lower():\n",
    "                    num_tests += 1\n",
    "                    passed_tests += 1\n",
    "\n",
    "            if num_tests > 0:\n",
    "                execution_score = passed_tests / num_tests\n",
    "\n",
    "        # Combine execution score and Critic's score (weighted average)\n",
    "        # Use weights from config\n",
    "        self.score = config[\"execution_weight\"] * execution_score + config[\"critic_weight\"] * self.critic_score\n",
    "        logging.info(f\"Node score calculated: {self.score}\")\n",
    "        return self.score\n",
    "\n",
    "    def upper_confidence_bound(self, exploration_weight: float = math.sqrt(2)) -> float:\n",
    "        if self.visits == 0:\n",
    "            return float('inf')\n",
    "        exploitation = self.value / self.visits\n",
    "        exploration = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "        return exploitation + exploration_weight * exploration\n",
    "\n",
    "    def backpropagate(self, reward: float, config: Dict) -> None:\n",
    "        \"\"\"Updates the node's value and propagates it to its ancestors.\"\"\"\n",
    "        logging.info(f\"Backpropagating reward: {reward} for node with strategy: {self.strategy}\")\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            if node.evaluation == \"Accept\":\n",
    "                node.score = 1\n",
    "            else:\n",
    "                node.calculate_score(config)  # Recalculate score during backpropagation\n",
    "            node = node.parent\n",
    "\n",
    "    def select_child(self) -> \"Node\":\n",
    "        \"\"\"Selects the best child using UCB.\"\"\"\n",
    "        logging.info(f\"Selecting child for node with strategy: {self.strategy}\")\n",
    "        best_child = None\n",
    "        best_ucb = float('-inf')\n",
    "        for child in self.children:\n",
    "            ucb = child.upper_confidence_bound()\n",
    "            if ucb > best_ucb:\n",
    "                best_ucb = ucb\n",
    "                best_child = child\n",
    "        logging.info(f\"Selected child with strategy: {best_child.strategy if best_child else 'None'}\")\n",
    "        return best_child\n",
    "\n",
    "    def is_terminal(self) -> bool:\n",
    "        return self.evaluation == \"Accept\"\n",
    "\n",
    "    def get_best_solution(self) -> \"Node\":\n",
    "        \"\"\"Retrieves the best solution from the subtree rooted at this node.\"\"\"\n",
    "        logging.info(f\"Getting best solution for node with strategy: {self.strategy}\")\n",
    "        all_nodes = [self] + self._get_all_children()\n",
    "        best_node = max(\n",
    "            all_nodes,\n",
    "            key=lambda node: (1 if node.evaluation == \"Accept\" else 0) * node.value\n",
    "        )\n",
    "        logging.info(f\"Best solution found: {best_node.solution}\")\n",
    "        return best_node\n",
    "\n",
    "    def _get_all_children(self) -> List[\"Node\"]:\n",
    "        \"\"\"Retrieves all children of this node (recursively).\"\"\"\n",
    "        all_nodes = []\n",
    "        queue = deque(self.children)\n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            all_nodes.append(node)\n",
    "            queue.extend(node.children)\n",
    "        return all_nodes\n",
    "\n",
    "class TreeState(TypedDict):\n",
    "    root: Optional[Node]\n",
    "    input: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc2255e5-cde3-4e9b-91d2-5da9a0aba354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tree Expansion Functions ---\n",
    "def generate_initial_strategies(state: TreeState, llm, client, necessary_metadata, config) -> dict:\n",
    "    \"\"\"Generates initial strategies and creates the root node.\"\"\"\n",
    "    question = state[\"input\"]\n",
    "    logging.info(f\"Generating initial strategies for question: {question}\")\n",
    "    thinker = Thinker(llm, necessary_metadata)\n",
    "    solver = Solver(llm, necessary_metadata)\n",
    "    critic = Critic(llm, client, necessary_metadata, config)\n",
    "\n",
    "    # Create a dummy root node\n",
    "    root = Node(strategy=\"Root\", solution=\"\", evaluation=\"Expand\", question=question, config=config)\n",
    "\n",
    "    # Generate and evaluate strategies one by one\n",
    "    strategies = []\n",
    "    for i in range(5):  # Assuming a maximum of 5 initial strategies as before\n",
    "        \n",
    "        if i == 0 :\n",
    "            # 1. Generate a strategy\n",
    "            strategy = thinker.generate_strategies(question, 1)\n",
    "            new_strategy = strategy[0]\n",
    "        else :\n",
    "            strategy = thinker.generate_strategies(question, 1, strategies)\n",
    "            new_strategy = strategy[0]\n",
    "            \n",
    "        if not new_strategy or new_strategy.lower() == \"no further strategies.\":\n",
    "            logging.info(\"No further strategies generated.\")\n",
    "            break\n",
    "\n",
    "        strategies.append(new_strategy)\n",
    "        logging.info(f\"Generated strategy {i+1}: {new_strategy}\")\n",
    "\n",
    "        # 2. Create a child node for the new strategy\n",
    "        child_node = Node(strategy=new_strategy, solution=\"\", evaluation=\"Expand\", parent=root, question=question,\n",
    "                          config=config)\n",
    "        root.children.append(child_node)\n",
    "\n",
    "        # 3. Generate a solution for the strategy\n",
    "        solution = solver.generate_solution(question, new_strategy)\n",
    "        child_node.solution = solution  # Update the node with the solution\n",
    "\n",
    "        # 4. Evaluate the solution\n",
    "        evaluation, critic_score, feedback = critic.evaluate_solution(question, solution, new_strategy)\n",
    "        child_node.evaluation = evaluation\n",
    "        child_node.critic_score = critic_score\n",
    "        child_node.calculate_score(config)\n",
    "\n",
    "        # 5. Check if the solution is verified (\"Accept\")\n",
    "        if evaluation == \"Accept\":\n",
    "            logging.info(f\"Verified solution found for strategy: {new_strategy}. Stopping initial strategy generation.\")\n",
    "            return {**state, \"root\": root}  # Stop early\n",
    "\n",
    "    logging.info(\"Finished generating initial strategies.\")\n",
    "    return {**state, \"root\": root}\n",
    "\n",
    "def expand_tree(state: TreeState, llm, client, necessary_metadata, config: Dict, max_depth: int = 5) -> dict:\n",
    "    \"\"\"Expands the tree based on node evaluations and scores.\"\"\"\n",
    "    root = state[\"root\"]\n",
    "    question = state[\"input\"]\n",
    "\n",
    "    # Check if an \"Accept\" node already exists before expanding\n",
    "    if any(node.evaluation == \"Accept\" for node in root._get_all_children()):\n",
    "        logging.info(\"Acceptable solution already exists. Skipping expansion.\")\n",
    "        return state\n",
    "\n",
    "    logging.info(f\"Expanding tree for question: {question}\")\n",
    "    current_node = select_node_to_expand(root)\n",
    "    logging.info(f\"Selected node for expansion: {current_node.strategy}\")\n",
    "\n",
    "    solver = Solver(llm, necessary_metadata)\n",
    "    critic = Critic(llm, client, necessary_metadata, config)\n",
    "    debugger = Debugger(llm, necessary_metadata)\n",
    "    thinker = Thinker(llm, necessary_metadata)\n",
    "\n",
    "    if current_node.evaluation == \"Expand\":\n",
    "        # Generate solution and evaluate\n",
    "        logging.info(f\"Generating solution for node: {current_node.strategy}\")\n",
    "        solution = solver.generate_solution(question, current_node.strategy)\n",
    "        logging.info(f\"Evaluating solution: {solution}\")\n",
    "        evaluation, critic_score, feedback = critic.evaluate_solution(question, solution, current_node.strategy)\n",
    "\n",
    "        # Update current node and create a new node\n",
    "        current_node.solution = solution\n",
    "        current_node.evaluation = evaluation\n",
    "        current_node.critic_score = critic_score\n",
    "        current_node.calculate_score(config)\n",
    "\n",
    "        new_node = Node(strategy=current_node.strategy, solution=solution, evaluation=evaluation,\n",
    "                        critic_score=critic_score, parent=current_node, question=question, config=config)\n",
    "        current_node.children.append(new_node)\n",
    "\n",
    "    elif current_node.evaluation == \"Refine\":\n",
    "        # Get reflections and refine solution\n",
    "        log = execute_sas_code(current_node.solution)\n",
    "        try:\n",
    "            error_reflection = critic._get_error_reflection(current_node.question, current_node.solution, log)\n",
    "            requirement_reflection = critic._get_requirement_reflection(current_node.question, current_node.solution,\n",
    "                                                                        log)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during reflections generation : {e}\")\n",
    "            raise\n",
    "\n",
    "        feedback = \"\"\n",
    "        if error_reflection.error_count > 0:\n",
    "            feedback += \"Errors:\\n\"\n",
    "            feedback += f\"- {error_reflection.error_log}\\n\"\n",
    "\n",
    "        if not requirement_reflection.requirements_met:\n",
    "            feedback += \"Missing Requirements:\\n\"\n",
    "            feedback += f\"- {requirement_reflection.missing_requirements}\\n\"\n",
    "\n",
    "        # Generate reflections using Thinker\n",
    "        logging.info(f\"Generating reflections for node: {current_node.strategy}\")\n",
    "        reflections = thinker.generate_reflections(current_node.question, current_node.solution, feedback, log,\n",
    "                                                   current_node.strategy)\n",
    "\n",
    "        if reflections == \"the code is good\":\n",
    "            # If no issues, simply append the current node again (no changes)\n",
    "            logging.info(f\"No issues found, appending current node again: {current_node.strategy}\")\n",
    "            evaluation, critic_score, critic_feedback = critic.evaluate_solution(\n",
    "                current_node.question, current_node.solution, current_node.strategy\n",
    "            )\n",
    "            new_node = Node(strategy=current_node.strategy, solution=current_node.solution, evaluation=evaluation,\n",
    "                            critic_score=critic_score, parent=current_node, question=question, config=config)\n",
    "            current_node.children.append(new_node)\n",
    "        else:\n",
    "            # Refine solution using Debugger\n",
    "            logging.info(f\"Refining solution for node: {current_node.strategy}\")\n",
    "            refined_solution = debugger.generate_refinement(question, current_node.solution, reflections)\n",
    "            logging.info(f\"Evaluating refined solution: {refined_solution}\")\n",
    "            evaluation, critic_score, feedback = critic.evaluate_solution(question, refined_solution,\n",
    "                                                                          current_node.strategy)\n",
    "\n",
    "            # Create new node for refined solution\n",
    "            new_node = Node(strategy=current_node.strategy, solution=refined_solution, evaluation=evaluation,\n",
    "                            critic_score=critic_score, parent=current_node, question=question, config=config)\n",
    "            current_node.children.append(new_node)\n",
    "\n",
    "            # Dynamic expansion based on Critic's score\n",
    "            if critic_score > config[\"expansion_threshold\"] and current_node.depth < max_depth:\n",
    "                logging.info(\n",
    "                    f\"Critic score {critic_score} above expansion threshold {config['expansion_threshold']}. Generating new strategies.\")\n",
    "                new_strategies = thinker.generate_strategies(question)\n",
    "                for new_strategy in new_strategies:\n",
    "                    new_node = Node(strategy=new_strategy, solution=\"\", evaluation=\"Expand\", parent=current_node,\n",
    "                                    question=question, config=config)\n",
    "                    current_node.children.append(new_node)\n",
    "            elif critic_score < config[\"abort_threshold\"]:\n",
    "                logging.info(\n",
    "                    f\"Critic score {critic_score} below abort threshold {config['abort_threshold']}. Marking node as Abort.\")\n",
    "                current_node.evaluation = \"Abort\"\n",
    "\n",
    "    return {**state, \"root\": root}\n",
    "\n",
    "def select_node_to_expand(root: Node) -> Node:\n",
    "    \"\"\"Selects a node to expand based on UCB and evaluation.\"\"\"\n",
    "    node = root\n",
    "    while node.children:\n",
    "        expandable_children = [child for child in node.children if child.evaluation == \"Expand\"]\n",
    "        if expandable_children:\n",
    "            logging.info(\"Selecting node to expand...\")\n",
    "            return max(expandable_children, key=lambda child: child.upper_confidence_bound())\n",
    "\n",
    "        refinable_children = [child for child in node.children if child.evaluation == \"Refine\"]\n",
    "        if refinable_children:\n",
    "            logging.info(\"Selecting node to refine...\")\n",
    "            return max(refinable_children, key=lambda child: child.upper_confidence_bound())\n",
    "\n",
    "        abortable_children = [child for child in node.children if child.evaluation == \"Abort\"]\n",
    "        if abortable_children:\n",
    "            logging.info(\"Found abortable node. Moving to parent.\")\n",
    "            if node == root:\n",
    "                return node\n",
    "            else:\n",
    "                node = node.parent\n",
    "        else:\n",
    "            node = node.children[0]\n",
    "\n",
    "    return node\n",
    "\n",
    "def should_continue(state: TreeState) -> str:\n",
    "    \"\"\"Determines whether the search should continue.\"\"\"\n",
    "    root = state[\"root\"]\n",
    "\n",
    "    # If a solution is found, stop\n",
    "    if any(node.evaluation == \"Accept\" for node in root._get_all_children()):\n",
    "        logging.info(\"Solution found. Stopping...\")\n",
    "        return END\n",
    "\n",
    "    # If any node is marked for expansion, continue\n",
    "    if any(node.evaluation == \"Expand\" for node in root._get_all_children()):\n",
    "        logging.info(\"Expansion required. Continuing...\")\n",
    "        return \"expand\"\n",
    "\n",
    "    # If any node is marked for expansion, continue\n",
    "    if any(node.evaluation == \"Refine\" for node in root._get_all_children()):\n",
    "        logging.info(\"Refinement required. Continuing...\")\n",
    "        return \"expand\"\n",
    "\n",
    "    # If max depth is reached for all nodes, stop\n",
    "    if all(node.depth >= 5 for node in root._get_all_children()):\n",
    "        logging.info(\"Max depth reached. Stopping...\")\n",
    "        return END\n",
    "\n",
    "    logging.info(\"No action determined. Continuing by default...\")\n",
    "    return \"expand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5858a-c56e-4f4b-866a-e921b040b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration (tune these parameters)\n",
    "config = {\n",
    "        \"execution_weight\": 0.6,\n",
    "        \"critic_weight\": 0.4,\n",
    "        \"error_weight\": 0.2,\n",
    "        \"requirement_weight\": 0.3,\n",
    "        \"verification_fail_score\": 0.5,\n",
    "        \"abort_threshold\": 0.4,\n",
    "        \"expansion_threshold\": 0.6\n",
    "    }\n",
    "    # Example query\n",
    "query = \"\"\"\n",
    "    \"\"\"\n",
    "# Initialize LLM and necessary metadata\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", convert_system_message_to_human=True)\n",
    "# Mocking the client for testing purposes, replace with actual client if needed\n",
    "client = Client(\"Qwen/Qwen2.5-Coder-Artifacts\")\n",
    "selector = MetadataSelector(model_name=\"gemini-2.0-flash-exp\", max_iterations=2)\n",
    "necesary_metadata = selector.select_metadata(query, filtered_metadata)\n",
    "print(\"necessary metadata to answer the question :\",necesary_metadata)\n",
    "# Initialize the tree state\n",
    "state = {\"input\": query}\n",
    "state = generate_initial_strategies(state, llm,client, necessary_metadata, config)\n",
    "\n",
    "# Main loop\n",
    "while should_continue(state) == \"expand\":\n",
    "    state = expand_tree(state, llm, client, necessary_metadata, config)\n",
    "\n",
    "# Find the best solution\n",
    "best_solution_node = state[\"root\"].get_best_solution()\n",
    "\n",
    "if best_solution_node:\n",
    "    logging.info(f\"Best solution found:\\n{best_solution_node.solution}\")\n",
    "    if best_solution_node.evaluation == \"Accept\":\n",
    "        logging.info(\"Solution was accepted by the Critic.\")\n",
    "    else:\n",
    "        logging.info(\"Solution was not accepted, but it's the best found within the search limits.\")\n",
    "\n",
    "    # Extract data if needed\n",
    "    extract_data(best_solution_node.solution,client)\n",
    "else:\n",
    "    logging.info(\"No solution found within the search limits.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
